{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"data-loaders/","text":"Data loaders solve the N+1 problem while loading data. The N+1 Problem Explained Say you query for a list of movies, and each movie includes some data about the director of the movie. Also assume that the Movie and Director entities are owned by two different services. In a na\u00efve implementation, to load 50 movies, you would have to call the Director service 50 times: once for each movie. This totals 51 queries: one query to get the list of movies, and 50 queries to get the director data for each movie. This obviously wouldn\u2019t perform very well. It would be much more efficient to create a list of directors to load, and load all of them at once in a single call. This first of all must be supported by the Director service , because that service needs to provide a way to load a list of Directors. The data fetchers in the Movie service need to be smart as well, to take care of batching the requests to the Directors service. This is where data loaders come in. What If My Service Doesn\u2019t Support Loading in Batches? What if (in this example) DirectorServiceClient doesn\u2019t provide a method to load a list of directors? What if it only provides a method to load a single director by ID? The same problem applies to REST services as well: what if there\u2019s no endpoint to load multiple directors? Similarly, to load from a database directly, you must write a query to load multiple directors. If such methods are unavailable, the providing service needs to fix this! Implementing a Data Loader The easiest way for you to register a data loader is for you to create a class that implements the org.dataloader.BatchLoader or org.dataloader.MappedBatchLoader interface. This interface is parameterized; it requires a type for the key and result of the BatchLoader . For example, if the identifiers for a Director are of type String , you could have a org.dataloader.BatchLoader<String, Director> . You must annotate the class with @DgsDataLoader so that the framework will register the data loader it represents. In order to implement the BatchLoader interface you must implement a CompletionStage<List<V>> load(List<K> keys) method. The following example is a data loader that loads data from an imaginary Director service: package com.netflix.graphql.dgs.example.dataLoader ; import com.netflix.graphql.dgs.DgsDataLoader ; import org.dataloader.BatchLoader ; import java.util.List ; import java.util.concurrent.CompletableFuture ; import java.util.concurrent.CompletionStage ; import java.util.stream.Collectors ; @DgsDataLoader ( name = \"directors\" ) public class DirectorsDataLoader implements BatchLoader < String , Director > { @Autowired DirectorServiceClient directorServiceClient ; @Override public CompletionStage < List < Director >> load ( List < String > keys ) { return CompletableFuture . supplyAsync (() -> directorServiceClient . loadDirectors ( keys )); } } The data loader is responsible for loading data for a given list of keys. In this example, it just passes on the list of keys to the backend that owns Director (this could for example be a [gRPC] service). However, you might also write such a service so that it loads data from a database. Although this example registers a data loader, nobody will use that data loader until you implement a data fetcher that uses it. Provide as Lambda Because BatchLoader is a functional interface (an interface with only a single method), you can also provide it as a lambda expression. Technically this is exactly the same as providing a class; it\u2019s really just another way of writing it: @DgsComponent public class ExampleBatchLoaderFromField { @DgsDataLoader ( name = \"directors\" ) public BatchLoader < String , Director > directorBatchLoader = keys -> CompletableFuture . supplyAsync (() -> directorServiceClient . loadDirectors ( keys )); } MappedBatchLoader The BatchLoader interface creates a List of values for a List of keys. You can also use the MappedBatchLoader which creates a Map of key/values for a Set of values. The latter is a better choice if you do not expect all keys to have a value. You register a MappedBatchLoader in the same way as you register a BatchLoader : @DgsDataLoader ( name = \"directors\" ) public class DirectorsDataLoader implements MappedBatchLoader < String , Director > { @Autowired DirectorServiceClient directorServiceClient ; @Override public CompletionStage < Map < String , Director >> load ( Set < String > keys ) { return CompletableFuture . supplyAsync (() -> directorServiceClient . loadDirectors ( keys )); } } Using a Data Loader The following is an example of a data fetcher that uses a data loader: @DgsComponent public class DirectorDataFetcher { @DgsData ( parentType = \"Movie\" , field = \"director\" ) public CompletableFuture < Director > director ( DataFetchingEnvironment dfe ) { DataLoader < String , Director > dataLoader = dfe . getDataLoader ( \"directors\" ); String id = dfe . getArgument ( \"directorId\" ); return dataLoader . load ( id ); } } The code above is mostly just a regular data fetcher. However, instead of actually loading the data from another service or database, it uses the data loader to do so. You can retrieve a data loader from the DataFetchingEnvironment with its getDataLoader() method. This requires you to pass the name of the data loader as a string. The other change to the data fetcher is that it returns a CompletableFuture instead of the actual type you\u2019re loading. This enables the framework to do work asynchronously, and is a requirement for batching . Using the DgsDataFetchingEnvironment You can also get the data loader in a type-safe way by using our custom DgsDataFetchingEnvironment , which is an enhanced version of DataFetchingEnvironment in graphql-java , and provides getDataLoader() using the classname. @DgsComponent public class DirectorDataFetcher { @DgsData ( parentType = \"Movie\" , field = \"director\" ) public CompletableFuture < Director > director ( DgsDataFetchingEnvironment dfe ) { DataLoader < String , Director > dataLoader = dfe . getDataLoader ( DirectorsDataLoader . class ); String id = dfe . getArgument ( \"directorId\" ); return dataLoader . load ( id ); } } The same works if you have @DgsDataLoader defined as a lambda instead of on a class as shown here . If you have multiple @DgsDataLoader lambdas defined as fields in the same class, you won't be able to use this feature. It is recommended that you use getDataLoader() with the loader name passed as a string in such cases. Note that there is no logic present about how batching works exactly; this is all handled by the framework! The framework will recognize that many directors need to be loaded when many movies are loaded, batch up all the calls to the data loader, and call the data loader with a list of IDs instead of a single ID. The data loader implemented above already knows how to handle a list of IDs, and that way it avoids the N+1 problem. Using Spring Features such as SsoCallerResolver inside a CompletableFuture When you write async data fetchers, the code will run on worker threads. Spring internally stores some context, for example to make the SsoCallerResolver work, on the thread context however. This context wouldn\u2019t be available inside code running on a different thread, which makes features such as SsoCallerResolver not work. Spring Boot has a solution for this: it manages a thread pool that does have this context carry over. You can inject this solution in the following way: @Autowired @DefaultExecutor private Executor executor ; You must pass in the executor as the second argument of the supplyAsync() method which is typically used to make data fetchers asynchronous. @DgsData ( parentType = \"Query\" , field = \"list_things\" ) public CompletableFuture < List < Thing >> resolve ( DataFetchingEnvironment environment ) { return CompletableFuture . supplyAsync (() -> { return myService . getThings (); }, executor ); Caching Batching is the most important aspect of preventing N+1 problems. Data loaders also support caching, however. If the same key is loaded multiple times, it will only be loaded once. For example, if a list of movies is loaded , and some movies are directed by the same director, the director data will only be retrieved once. !!!info \"Caching is Disabled by Default in DGS 1\" Version 1 of the DGS framework disables caching by default, but you can switch it on in the @DgsDataLoader annotation: @DgsDataLoader ( name = \"directors\" , caching = true ) class DirectorsBatchLoader implements BatchLoader < String , Director > {} You do not need to make this change in version 2 of the DGS framework, because that version enables caching by default. Batch Size Sometimes it\u2019s possible to load multiple items at once, but to a certain limit. When loading from a database for example, an IN query could be used , but maybe with the limitation of a maximum number of IDs to provide. The @DgsDataLoader has a maxBatchSize annotation that you can use to configure this behavior. By default it does not specify a maximum batch size. Data Loader Scope Data loaders are wired up to only span a single request. This is what most use cases require. Spanning multiple requests can introduce difficult-to-debug issues.","title":"Async Data Fetching"},{"location":"data-loaders/#the-n1-problem-explained","text":"Say you query for a list of movies, and each movie includes some data about the director of the movie. Also assume that the Movie and Director entities are owned by two different services. In a na\u00efve implementation, to load 50 movies, you would have to call the Director service 50 times: once for each movie. This totals 51 queries: one query to get the list of movies, and 50 queries to get the director data for each movie. This obviously wouldn\u2019t perform very well. It would be much more efficient to create a list of directors to load, and load all of them at once in a single call. This first of all must be supported by the Director service , because that service needs to provide a way to load a list of Directors. The data fetchers in the Movie service need to be smart as well, to take care of batching the requests to the Directors service. This is where data loaders come in.","title":"The N+1 Problem Explained"},{"location":"data-loaders/#what-if-my-service-doesnt-support-loading-in-batches","text":"What if (in this example) DirectorServiceClient doesn\u2019t provide a method to load a list of directors? What if it only provides a method to load a single director by ID? The same problem applies to REST services as well: what if there\u2019s no endpoint to load multiple directors? Similarly, to load from a database directly, you must write a query to load multiple directors. If such methods are unavailable, the providing service needs to fix this!","title":"What If My Service Doesn\u2019t Support Loading in Batches?"},{"location":"data-loaders/#implementing-a-data-loader","text":"The easiest way for you to register a data loader is for you to create a class that implements the org.dataloader.BatchLoader or org.dataloader.MappedBatchLoader interface. This interface is parameterized; it requires a type for the key and result of the BatchLoader . For example, if the identifiers for a Director are of type String , you could have a org.dataloader.BatchLoader<String, Director> . You must annotate the class with @DgsDataLoader so that the framework will register the data loader it represents. In order to implement the BatchLoader interface you must implement a CompletionStage<List<V>> load(List<K> keys) method. The following example is a data loader that loads data from an imaginary Director service: package com.netflix.graphql.dgs.example.dataLoader ; import com.netflix.graphql.dgs.DgsDataLoader ; import org.dataloader.BatchLoader ; import java.util.List ; import java.util.concurrent.CompletableFuture ; import java.util.concurrent.CompletionStage ; import java.util.stream.Collectors ; @DgsDataLoader ( name = \"directors\" ) public class DirectorsDataLoader implements BatchLoader < String , Director > { @Autowired DirectorServiceClient directorServiceClient ; @Override public CompletionStage < List < Director >> load ( List < String > keys ) { return CompletableFuture . supplyAsync (() -> directorServiceClient . loadDirectors ( keys )); } } The data loader is responsible for loading data for a given list of keys. In this example, it just passes on the list of keys to the backend that owns Director (this could for example be a [gRPC] service). However, you might also write such a service so that it loads data from a database. Although this example registers a data loader, nobody will use that data loader until you implement a data fetcher that uses it.","title":"Implementing a Data Loader"},{"location":"data-loaders/#provide-as-lambda","text":"Because BatchLoader is a functional interface (an interface with only a single method), you can also provide it as a lambda expression. Technically this is exactly the same as providing a class; it\u2019s really just another way of writing it: @DgsComponent public class ExampleBatchLoaderFromField { @DgsDataLoader ( name = \"directors\" ) public BatchLoader < String , Director > directorBatchLoader = keys -> CompletableFuture . supplyAsync (() -> directorServiceClient . loadDirectors ( keys )); }","title":"Provide as Lambda"},{"location":"data-loaders/#mappedbatchloader","text":"The BatchLoader interface creates a List of values for a List of keys. You can also use the MappedBatchLoader which creates a Map of key/values for a Set of values. The latter is a better choice if you do not expect all keys to have a value. You register a MappedBatchLoader in the same way as you register a BatchLoader : @DgsDataLoader ( name = \"directors\" ) public class DirectorsDataLoader implements MappedBatchLoader < String , Director > { @Autowired DirectorServiceClient directorServiceClient ; @Override public CompletionStage < Map < String , Director >> load ( Set < String > keys ) { return CompletableFuture . supplyAsync (() -> directorServiceClient . loadDirectors ( keys )); } }","title":"MappedBatchLoader"},{"location":"data-loaders/#using-a-data-loader","text":"The following is an example of a data fetcher that uses a data loader: @DgsComponent public class DirectorDataFetcher { @DgsData ( parentType = \"Movie\" , field = \"director\" ) public CompletableFuture < Director > director ( DataFetchingEnvironment dfe ) { DataLoader < String , Director > dataLoader = dfe . getDataLoader ( \"directors\" ); String id = dfe . getArgument ( \"directorId\" ); return dataLoader . load ( id ); } } The code above is mostly just a regular data fetcher. However, instead of actually loading the data from another service or database, it uses the data loader to do so. You can retrieve a data loader from the DataFetchingEnvironment with its getDataLoader() method. This requires you to pass the name of the data loader as a string. The other change to the data fetcher is that it returns a CompletableFuture instead of the actual type you\u2019re loading. This enables the framework to do work asynchronously, and is a requirement for batching .","title":"Using a Data Loader"},{"location":"data-loaders/#using-the-dgsdatafetchingenvironment","text":"You can also get the data loader in a type-safe way by using our custom DgsDataFetchingEnvironment , which is an enhanced version of DataFetchingEnvironment in graphql-java , and provides getDataLoader() using the classname. @DgsComponent public class DirectorDataFetcher { @DgsData ( parentType = \"Movie\" , field = \"director\" ) public CompletableFuture < Director > director ( DgsDataFetchingEnvironment dfe ) { DataLoader < String , Director > dataLoader = dfe . getDataLoader ( DirectorsDataLoader . class ); String id = dfe . getArgument ( \"directorId\" ); return dataLoader . load ( id ); } } The same works if you have @DgsDataLoader defined as a lambda instead of on a class as shown here . If you have multiple @DgsDataLoader lambdas defined as fields in the same class, you won't be able to use this feature. It is recommended that you use getDataLoader() with the loader name passed as a string in such cases. Note that there is no logic present about how batching works exactly; this is all handled by the framework! The framework will recognize that many directors need to be loaded when many movies are loaded, batch up all the calls to the data loader, and call the data loader with a list of IDs instead of a single ID. The data loader implemented above already knows how to handle a list of IDs, and that way it avoids the N+1 problem.","title":"Using the DgsDataFetchingEnvironment"},{"location":"data-loaders/#using-spring-features-such-as-ssocallerresolver-inside-a-completablefuture","text":"When you write async data fetchers, the code will run on worker threads. Spring internally stores some context, for example to make the SsoCallerResolver work, on the thread context however. This context wouldn\u2019t be available inside code running on a different thread, which makes features such as SsoCallerResolver not work. Spring Boot has a solution for this: it manages a thread pool that does have this context carry over. You can inject this solution in the following way: @Autowired @DefaultExecutor private Executor executor ; You must pass in the executor as the second argument of the supplyAsync() method which is typically used to make data fetchers asynchronous. @DgsData ( parentType = \"Query\" , field = \"list_things\" ) public CompletableFuture < List < Thing >> resolve ( DataFetchingEnvironment environment ) { return CompletableFuture . supplyAsync (() -> { return myService . getThings (); }, executor );","title":"Using Spring Features such as SsoCallerResolver inside a CompletableFuture"},{"location":"data-loaders/#caching","text":"Batching is the most important aspect of preventing N+1 problems. Data loaders also support caching, however. If the same key is loaded multiple times, it will only be loaded once. For example, if a list of movies is loaded , and some movies are directed by the same director, the director data will only be retrieved once. !!!info \"Caching is Disabled by Default in DGS 1\" Version 1 of the DGS framework disables caching by default, but you can switch it on in the @DgsDataLoader annotation: @DgsDataLoader ( name = \"directors\" , caching = true ) class DirectorsBatchLoader implements BatchLoader < String , Director > {} You do not need to make this change in version 2 of the DGS framework, because that version enables caching by default.","title":"Caching"},{"location":"data-loaders/#batch-size","text":"Sometimes it\u2019s possible to load multiple items at once, but to a certain limit. When loading from a database for example, an IN query could be used , but maybe with the limitation of a maximum number of IDs to provide. The @DgsDataLoader has a maxBatchSize annotation that you can use to configure this behavior. By default it does not specify a maximum batch size.","title":"Batch Size"},{"location":"data-loaders/#data-loader-scope","text":"Data loaders are wired up to only span a single request. This is what most use cases require. Spanning multiple requests can introduce difficult-to-debug issues.","title":"Data Loader Scope"},{"location":"error-handling/","text":"It is common in GraphQL to support error reporting by adding an errors block to a response. Responses can contain both data and errors, for example when some fields where resolved successfully, but other fields had errors. A field with an error is set to null, and an error is added to the errors block. The DGS framework has an exception handler out-of-the-box that works according to the specification described in the Error Specification section on this page. This exception handler handles exceptions from data fetchers. Any RuntimeException is translated to a GraphQLError of type INTERNAL . For some specific exception types, a more specific GraphQL error type is used. Exception type GraphQL error type description AccessDeniedException PERMISSION_DENIED When a @Secured check fails DgsEntityNotFoundException NOT_FOUND Thrown by the developer when a requested entity (e.g. based on query parameters) isn't found Mapping custom exceptions It can be useful to map application specific exceptions to meaningful exceptions back to the client. You can do this by registering a DataFetcherExceptionHandler . Make sure to either extend or delegate to the DefaultDataFetcherExceptionHandler class, this is the default exception handler of the framework. If you don't extend/delegate to this class, you lose the framework's built-in exception handler. The following is an example of a custom exception handler implementation. @Component public class CustomDataFetchingExceptionHandler implements DataFetcherExceptionHandler { private final DefaultDataFetcherExceptionHandler defaultHandler = new DefaultDataFetcherExceptionHandler (); @Override public DataFetcherExceptionHandlerResult onException ( DataFetcherExceptionHandlerParameters handlerParameters ) { if ( handlerParameters . getException () instanceof MyException ) { Map < String , Object > debugInfo = new HashMap <> (); debugInfo . put ( \"somefield\" , \"somevalue\" ); GraphQLError graphqlError = TypedGraphQLError . INTERNAL . message ( \"This custom thing went wrong!\" ) . debugInfo ( debugInfo ) . path ( handlerParameters . getPath ()). build (); return DataFetcherExceptionHandlerResult . newResult () . error ( graphqlError ) . build (); } else { return defaultHandler . onException ( handlerParameters ); } } } The following data fetcher throws MyException . @DgsComponent public class HelloDataFetcher { @DgsData ( parentType = \"Query\" , field = \"hello\" ) @DgsEnableDataFetcherInstrumentation ( false ) public String hello ( DataFetchingEnvironment dfe ) { throw new MyException (); } } Querying the hello field results in the following response. { \"errors\" : [ { \"message\" : \"This custom thing went wrong!\" , \"locations\" : [], \"path\" : [ \"hello\" ], \"extensions\" : { \"errorType\" : \"INTERNAL\" , \"debugInfo\" : { \"somefield\" : \"somevalue\" } } } ], \"data\" : { \"hello\" : null } } Error specification There are two families of errors we typically encounter for GraphQL: Comprehensive Errors. These are unexpected errors and do not represent a condition that the end user can be expected to fix. Errors of this sort are generally applicable to many types and fields. Such errors appear in the errors array in the GraphQL response. Errors as Data. These are errors that are informative to the end user (for example: \u201cthis title is not available in your country\u201d or \u201cyour account has been suspended\u201d). Errors of this sort are typically specific to a particular use case and apply only to certain fields or to a certain subset of fields. These errors are part of the GraphQL schema. The GraphQLError Interface The GraphQL specification provides minimal guidance on the structure of an error. The only required field is a message String, which has no defined format. In Studio Edge we would like to have a stronger, more expressive contract. Here is the definition we are using: field type description message (non-nullable) String! a string description of the error intended for the developer as a guide to understand and correct the error locations [Location] an array of code locations, where each location is a map with the keys line and column , both natural numbers starting from 1 that describe the beginning of an associated syntax element path [String | Int] if the error is associated with one or more particular fields in the response, this field of the error details the paths of those response fields that experienced the error (this allows clients to identify whether a null result is intentional or caused by a runtime error) extensions [TypedError] see \u201cThe TypedError Interface\u201d below \"\"\" Error format as defined in GraphQL Spec \"\"\" interface GraphQLError { message: String! // Required by GraphQL Spec locations: [Location] // See GraphQL Spec path: [String | Int] // See GraphQL Spec extensions: TypedError } See the GraphQL specification: Errors for more information. The TypedError Interface Studio Edge defines TypedError as follows: field type description errorType (non-nullable) ErrorType! an enumerated error code that is meant as a fairly coarse characterization of an error, sufficient for client-side branching logic errorDetail ErrorDetail an enumeration that provides more detail about the error, including its specific cause (the elements of this enumeration are subject to change and are not documented here) origin String the name of the source that issued the error (for instance the name of a backend service, [DGS], gateway, client library, or client app) debugInfo DebugInfo if the request included a flag indicating that it wanted debug information, this field contains that additional information (such as a stack trace or additional reporting from an upstream service) debugUri String the URI of a page that contains additional information that may be helpful in debugging the error (this could be a generic page for errors of this sort, or a specific page about the particular error instance) interface TypedError { \"\"\" An error code from the ErrorType enumeration. An errorType is a fairly coarse characterization of an error that should be sufficient for client side branching logic. \"\"\" errorType: ErrorType! \"\"\" The ErrorDetail is an optional field which will provide more fine grained information on the error condition. This allows the ErrorType enumeration to be small and mostly static so that application branching logic can depend on it. The ErrorDetail provides a more specific cause for the error. This enumeration will be much larger and likely change/grow over time. \"\"\" errorDetail: ErrorDetail \"\"\" Indicates the source that issued the error. For example, could be a backend service name, a domain graph service name, or a gateway. In the case of client code throwing the error, this may be a client library name, or the client app name. \"\"\" origin: String \"\"\" Optionally provided based on request flag Could include e.g. stacktrace or info from upstream service \"\"\" debugInfo: DebugInfo \"\"\" Http URI to a page detailing additional information that could be used to debug the error. This information may be general to the class of error or specific to this particular instance of the error. \"\"\" debugUri: String } The ErrorType Enumeration The following table shows the available ErrorType enum values: type description HTTP analog BAD_REQUEST This indicates a problem with the request. Retrying the same request is not likely to succeed. An example would be a query or argument that cannot be deserialized. 400 Bad Request FAILED_PRECONDITION The operation was rejected because the system is not in a state required for the operation\u2019s execution. For example, the directory to be deleted is non-empty, an rmdir operation is applied to a non-directory, etc. Use UNAVAILABLE instead if the client can retry just the failing call without waiting for the system state to be explicitly fixed. 400 Bad Request, or 500 Internal Server Error INTERNAL This indicates that an unexpected internal error was encountered: some invariants expected by the underlying system have been broken. This error code is reserved for serious errors. 500 Internal Server Error NOT_FOUND This could apply to a resource that has never existed (e.g. bad resource id), or a resource that no longer exists (e.g. cache expired). Note to server developers: if a request is denied for an entire class of users, such as gradual feature rollout or undocumented allowlist, NOT_FOUND may be used. If a request is denied for some users within a class of users, such as user-based access control, PERMISSION_DENIED must be used. 404 Not Found PERMISSION_DENIED This indicates that the requester does not have permission to execute the specified operation. PERMISSION_DENIED must not be used for rejections caused by exhausting some resource or quota. PERMISSION_DENIED must not be used if the caller cannot be identified (use UNAUTHENTICATED instead for those errors). This error does not imply that the request is valid or the requested entity exists or satisfies other pre-conditions. 403 Forbidden UNAUTHENTICATED This indicates that the request does not have valid authentication credentials but the route requires authentication. 401 Unauthorized UNAVAILABLE This indicates that the service is currently unavailable. This is most likely a transient condition, which can be corrected by retrying with a backoff. 503 Unavailable UNKNOWN This error may be returned, for example, when an error code received from another address space belongs to an error space that is not known in this address space. Errors raised by APIs that do not return enough error information may also be converted to this error. If a client sees an errorType that is not known to it, it will be interpreted as UNKNOWN . Unknown errors must not trigger any special behavior. They may be treated by an implementation as being equivalent to INTERNAL . 520 Unknown Error The HTTP analogs are only rough mappings that are given here to provide a quick conceptual explanation of the semantics of the error by showing their analogs in the HTTP specification.","title":"Error Handling"},{"location":"error-handling/#mapping-custom-exceptions","text":"It can be useful to map application specific exceptions to meaningful exceptions back to the client. You can do this by registering a DataFetcherExceptionHandler . Make sure to either extend or delegate to the DefaultDataFetcherExceptionHandler class, this is the default exception handler of the framework. If you don't extend/delegate to this class, you lose the framework's built-in exception handler. The following is an example of a custom exception handler implementation. @Component public class CustomDataFetchingExceptionHandler implements DataFetcherExceptionHandler { private final DefaultDataFetcherExceptionHandler defaultHandler = new DefaultDataFetcherExceptionHandler (); @Override public DataFetcherExceptionHandlerResult onException ( DataFetcherExceptionHandlerParameters handlerParameters ) { if ( handlerParameters . getException () instanceof MyException ) { Map < String , Object > debugInfo = new HashMap <> (); debugInfo . put ( \"somefield\" , \"somevalue\" ); GraphQLError graphqlError = TypedGraphQLError . INTERNAL . message ( \"This custom thing went wrong!\" ) . debugInfo ( debugInfo ) . path ( handlerParameters . getPath ()). build (); return DataFetcherExceptionHandlerResult . newResult () . error ( graphqlError ) . build (); } else { return defaultHandler . onException ( handlerParameters ); } } } The following data fetcher throws MyException . @DgsComponent public class HelloDataFetcher { @DgsData ( parentType = \"Query\" , field = \"hello\" ) @DgsEnableDataFetcherInstrumentation ( false ) public String hello ( DataFetchingEnvironment dfe ) { throw new MyException (); } } Querying the hello field results in the following response. { \"errors\" : [ { \"message\" : \"This custom thing went wrong!\" , \"locations\" : [], \"path\" : [ \"hello\" ], \"extensions\" : { \"errorType\" : \"INTERNAL\" , \"debugInfo\" : { \"somefield\" : \"somevalue\" } } } ], \"data\" : { \"hello\" : null } }","title":"Mapping custom exceptions"},{"location":"error-handling/#error-specification","text":"There are two families of errors we typically encounter for GraphQL: Comprehensive Errors. These are unexpected errors and do not represent a condition that the end user can be expected to fix. Errors of this sort are generally applicable to many types and fields. Such errors appear in the errors array in the GraphQL response. Errors as Data. These are errors that are informative to the end user (for example: \u201cthis title is not available in your country\u201d or \u201cyour account has been suspended\u201d). Errors of this sort are typically specific to a particular use case and apply only to certain fields or to a certain subset of fields. These errors are part of the GraphQL schema.","title":"Error specification"},{"location":"error-handling/#the-graphqlerror-interface","text":"The GraphQL specification provides minimal guidance on the structure of an error. The only required field is a message String, which has no defined format. In Studio Edge we would like to have a stronger, more expressive contract. Here is the definition we are using: field type description message (non-nullable) String! a string description of the error intended for the developer as a guide to understand and correct the error locations [Location] an array of code locations, where each location is a map with the keys line and column , both natural numbers starting from 1 that describe the beginning of an associated syntax element path [String | Int] if the error is associated with one or more particular fields in the response, this field of the error details the paths of those response fields that experienced the error (this allows clients to identify whether a null result is intentional or caused by a runtime error) extensions [TypedError] see \u201cThe TypedError Interface\u201d below \"\"\" Error format as defined in GraphQL Spec \"\"\" interface GraphQLError { message: String! // Required by GraphQL Spec locations: [Location] // See GraphQL Spec path: [String | Int] // See GraphQL Spec extensions: TypedError } See the GraphQL specification: Errors for more information.","title":"The GraphQLError Interface"},{"location":"error-handling/#the-typederror-interface","text":"Studio Edge defines TypedError as follows: field type description errorType (non-nullable) ErrorType! an enumerated error code that is meant as a fairly coarse characterization of an error, sufficient for client-side branching logic errorDetail ErrorDetail an enumeration that provides more detail about the error, including its specific cause (the elements of this enumeration are subject to change and are not documented here) origin String the name of the source that issued the error (for instance the name of a backend service, [DGS], gateway, client library, or client app) debugInfo DebugInfo if the request included a flag indicating that it wanted debug information, this field contains that additional information (such as a stack trace or additional reporting from an upstream service) debugUri String the URI of a page that contains additional information that may be helpful in debugging the error (this could be a generic page for errors of this sort, or a specific page about the particular error instance) interface TypedError { \"\"\" An error code from the ErrorType enumeration. An errorType is a fairly coarse characterization of an error that should be sufficient for client side branching logic. \"\"\" errorType: ErrorType! \"\"\" The ErrorDetail is an optional field which will provide more fine grained information on the error condition. This allows the ErrorType enumeration to be small and mostly static so that application branching logic can depend on it. The ErrorDetail provides a more specific cause for the error. This enumeration will be much larger and likely change/grow over time. \"\"\" errorDetail: ErrorDetail \"\"\" Indicates the source that issued the error. For example, could be a backend service name, a domain graph service name, or a gateway. In the case of client code throwing the error, this may be a client library name, or the client app name. \"\"\" origin: String \"\"\" Optionally provided based on request flag Could include e.g. stacktrace or info from upstream service \"\"\" debugInfo: DebugInfo \"\"\" Http URI to a page detailing additional information that could be used to debug the error. This information may be general to the class of error or specific to this particular instance of the error. \"\"\" debugUri: String }","title":"The TypedError Interface"},{"location":"error-handling/#the-errortype-enumeration","text":"The following table shows the available ErrorType enum values: type description HTTP analog BAD_REQUEST This indicates a problem with the request. Retrying the same request is not likely to succeed. An example would be a query or argument that cannot be deserialized. 400 Bad Request FAILED_PRECONDITION The operation was rejected because the system is not in a state required for the operation\u2019s execution. For example, the directory to be deleted is non-empty, an rmdir operation is applied to a non-directory, etc. Use UNAVAILABLE instead if the client can retry just the failing call without waiting for the system state to be explicitly fixed. 400 Bad Request, or 500 Internal Server Error INTERNAL This indicates that an unexpected internal error was encountered: some invariants expected by the underlying system have been broken. This error code is reserved for serious errors. 500 Internal Server Error NOT_FOUND This could apply to a resource that has never existed (e.g. bad resource id), or a resource that no longer exists (e.g. cache expired). Note to server developers: if a request is denied for an entire class of users, such as gradual feature rollout or undocumented allowlist, NOT_FOUND may be used. If a request is denied for some users within a class of users, such as user-based access control, PERMISSION_DENIED must be used. 404 Not Found PERMISSION_DENIED This indicates that the requester does not have permission to execute the specified operation. PERMISSION_DENIED must not be used for rejections caused by exhausting some resource or quota. PERMISSION_DENIED must not be used if the caller cannot be identified (use UNAUTHENTICATED instead for those errors). This error does not imply that the request is valid or the requested entity exists or satisfies other pre-conditions. 403 Forbidden UNAUTHENTICATED This indicates that the request does not have valid authentication credentials but the route requires authentication. 401 Unauthorized UNAVAILABLE This indicates that the service is currently unavailable. This is most likely a transient condition, which can be corrected by retrying with a backoff. 503 Unavailable UNKNOWN This error may be returned, for example, when an error code received from another address space belongs to an error space that is not known in this address space. Errors raised by APIs that do not return enough error information may also be converted to this error. If a client sees an errorType that is not known to it, it will be interpreted as UNKNOWN . Unknown errors must not trigger any special behavior. They may be treated by an implementation as being equivalent to INTERNAL . 520 Unknown Error The HTTP analogs are only rough mappings that are given here to provide a quick conceptual explanation of the semantics of the error by showing their analogs in the HTTP specification.","title":"The ErrorType Enumeration"},{"location":"examples/","text":"We have several example applications that demonstrate the use of the DGS framework in a fully working example. Each example has detailed documentation in the corresponding Github repository, and is further explained in code comments. Java DGS example : An implementation of a typical GraphQL service Kotlin DGS example : The same example as above, but implemented in Kotlin Federation examples : A Federated GraphQL example, using Apollo Gateway","title":"Examples"},{"location":"generating-code-from-schema/","text":"The DGS Code Generation plugin generates code during your project\u2019s build process based on your Domain Graph Service\u2019s GraphQL schema file. The plugin generates the following: Data types for types, input types, enums and interfaces. A DgsConstants class containing the names of types and fields Example data fetchers A type safe query API that represents your queries Quick Start To apply the plugin, update your project\u2019s build.gradle file to include the following: // Using plugins DSL plugins { id \"com.netflix.dgs.codegen\" version \"4.0.10\" } Alternatively, you can set up classpath dependencies in your buildscript: buildscript { dependencies { classpath 'com.netflix.graphql.dgs.codegen:graphql-dgs-codegen-gradle:latest.release' } } apply plugin: 'com.netflix.dgs.codegen' generateJava { schemaPaths = [ \"${projectDir}/src/main/resources/schema\" ] // List of directories containing schema files packageName = 'com.example.packagename' // The package name to use to generate sources generateClient = true // Enable generating the type safe query API } The plugin adds a generateJava Gradle task that runs as part of your project\u2019s build. generateJava generates the code in the project\u2019s build/generated directory. This folder is automatically added to the project's classpath. Types are available as part of the package specified by the packageName .types , where you specify the value of packageName as a configuration in your build.gradle file. Please ensure that your project\u2019s sources refer to the generated code using the specified package name. generateJava generates the data fetchers and places them in build/generated-examples . NOTE: generateJava does NOT add the data fetchers that it generates to your project\u2019s sources. These fetchers serve mainly as a basic boilerplate code that require further implementation from you. You can exclude parts of the schema from code-generation by placing them in a different schema directory that is not specified as part of the schemaPaths for the plugin. Mapping existing types Codegen tries to generate a type for each type it finds in the schema, with a few exceptions. Basic scalar types - are mapped to corresponding Java/Kotlin types (String, Integer etc.) Data and time types - are mapped to corresponding java.time classes PageInfo and RelayPageInfo - are mapped to graphql.relay classes Types mapped with a typeMapping configuration When you have existing classes that you want to use instead of generating a class for a certain type, you can configure the plugin to do so using a typeMapping . The typeMapping configuration is a Map where each key is a GraphQL type and each value is a fully qualified Java/Kotlin type. generateJava { typeMapping = [ \"MyGraphQLType\" : \"com.mypackage.MyJavaType\" ] } Generating Client APIs The code generator can also create client API classes. You can use these classes to query data from a GraphQL endpoint using Java. Java GraphQL clients are useful for server-to-server communication and testing. Code generation creates a field-name GraphQLQuery for each Query and Mutation field. The *GraphQLQuery query class contains fields for each parameter of the field. For each type returned by a Query or Mutation, code generation creates a *Projection . A projection is a builder class that specifies which fields get returned. The following is an example usage of a generated API: GraphQLQueryRequest graphQLQueryRequest = new GraphQLQueryRequest ( new TicksGraphQLQuery . Builder () . first ( first ) . after ( after ) . build (), new TicksConnectionProjection () . edges () . node () . date () . route () . name () . votes () . starRating () . parent () . grade ()); This API was generated based on the following schema. The edges and node types are because the schema uses pagination. The API allows for a fluent style of writing queries, with almost the same feel of writing the query as a String, but with the added benefit of code completion and type safety. type Query @extends { ticks(first: Int, after: Int, allowCached: Boolean): TicksConnection } type Tick { id: ID route: Route date: LocalDate userStars: Int userRating: String leadStyle: LeadStyle comments: String } type Votes { starRating: Float nrOfVotes: Int } type Route { routeId: ID name: String grade: String style: Style pitches: Int votes: Votes location: [String] } type TicksConnection { edges: [TickEdge] } type TickEdge { cursor: String! node: Tick } Sending a Query A GraphQLQueryRequest can be serialized to JSON and sent to a GraphQL endpoint. The following example uses RestTemplate with Metatron. @Metatron ( \"spinnaker-app-name-goes-here\" ) private RestTemplate dgsRestTemplate ; private ObjectMapper mapper = new ObjectMapper (); private static HttpEntity < String > httpEntity ( String request ) { HttpHeaders headers = new HttpHeaders (); headers . setAccept ( Collections . singletonList ( MediaType . APPLICATION_JSON )); headers . setContentType ( MediaType . APPLICATION_JSON ); return new HttpEntity <> ( request , headers ); } Map < String , String > request = Collections . singletonMap ( \"query\" , graphQLQueryRequest . serialize ()); // Invoke REST call, and get the \"ticks\" from data. JsonNode node = dgsRestTemplate . exchange ( URL , HttpMethod . POST , httpEntity ( mapper . writeValueAsString ( request )), new ParameterizedTypeReference < JsonNode > () { }). getBody (). get ( \"data\" ). get ( \"ticks\" ); //Convert to the response type TicksConnection ticks = mapper . convertValue ( node , TicksConnection . class );","title":"Code Generation"},{"location":"generating-code-from-schema/#quick-start","text":"To apply the plugin, update your project\u2019s build.gradle file to include the following: // Using plugins DSL plugins { id \"com.netflix.dgs.codegen\" version \"4.0.10\" } Alternatively, you can set up classpath dependencies in your buildscript: buildscript { dependencies { classpath 'com.netflix.graphql.dgs.codegen:graphql-dgs-codegen-gradle:latest.release' } } apply plugin: 'com.netflix.dgs.codegen' generateJava { schemaPaths = [ \"${projectDir}/src/main/resources/schema\" ] // List of directories containing schema files packageName = 'com.example.packagename' // The package name to use to generate sources generateClient = true // Enable generating the type safe query API } The plugin adds a generateJava Gradle task that runs as part of your project\u2019s build. generateJava generates the code in the project\u2019s build/generated directory. This folder is automatically added to the project's classpath. Types are available as part of the package specified by the packageName .types , where you specify the value of packageName as a configuration in your build.gradle file. Please ensure that your project\u2019s sources refer to the generated code using the specified package name. generateJava generates the data fetchers and places them in build/generated-examples . NOTE: generateJava does NOT add the data fetchers that it generates to your project\u2019s sources. These fetchers serve mainly as a basic boilerplate code that require further implementation from you. You can exclude parts of the schema from code-generation by placing them in a different schema directory that is not specified as part of the schemaPaths for the plugin.","title":"Quick Start"},{"location":"generating-code-from-schema/#mapping-existing-types","text":"Codegen tries to generate a type for each type it finds in the schema, with a few exceptions. Basic scalar types - are mapped to corresponding Java/Kotlin types (String, Integer etc.) Data and time types - are mapped to corresponding java.time classes PageInfo and RelayPageInfo - are mapped to graphql.relay classes Types mapped with a typeMapping configuration When you have existing classes that you want to use instead of generating a class for a certain type, you can configure the plugin to do so using a typeMapping . The typeMapping configuration is a Map where each key is a GraphQL type and each value is a fully qualified Java/Kotlin type. generateJava { typeMapping = [ \"MyGraphQLType\" : \"com.mypackage.MyJavaType\" ] }","title":"Mapping existing types"},{"location":"generating-code-from-schema/#generating-client-apis","text":"The code generator can also create client API classes. You can use these classes to query data from a GraphQL endpoint using Java. Java GraphQL clients are useful for server-to-server communication and testing. Code generation creates a field-name GraphQLQuery for each Query and Mutation field. The *GraphQLQuery query class contains fields for each parameter of the field. For each type returned by a Query or Mutation, code generation creates a *Projection . A projection is a builder class that specifies which fields get returned. The following is an example usage of a generated API: GraphQLQueryRequest graphQLQueryRequest = new GraphQLQueryRequest ( new TicksGraphQLQuery . Builder () . first ( first ) . after ( after ) . build (), new TicksConnectionProjection () . edges () . node () . date () . route () . name () . votes () . starRating () . parent () . grade ()); This API was generated based on the following schema. The edges and node types are because the schema uses pagination. The API allows for a fluent style of writing queries, with almost the same feel of writing the query as a String, but with the added benefit of code completion and type safety. type Query @extends { ticks(first: Int, after: Int, allowCached: Boolean): TicksConnection } type Tick { id: ID route: Route date: LocalDate userStars: Int userRating: String leadStyle: LeadStyle comments: String } type Votes { starRating: Float nrOfVotes: Int } type Route { routeId: ID name: String grade: String style: Style pitches: Int votes: Votes location: [String] } type TicksConnection { edges: [TickEdge] } type TickEdge { cursor: String! node: Tick }","title":"Generating Client APIs"},{"location":"generating-code-from-schema/#sending-a-query","text":"A GraphQLQueryRequest can be serialized to JSON and sent to a GraphQL endpoint. The following example uses RestTemplate with Metatron. @Metatron ( \"spinnaker-app-name-goes-here\" ) private RestTemplate dgsRestTemplate ; private ObjectMapper mapper = new ObjectMapper (); private static HttpEntity < String > httpEntity ( String request ) { HttpHeaders headers = new HttpHeaders (); headers . setAccept ( Collections . singletonList ( MediaType . APPLICATION_JSON )); headers . setContentType ( MediaType . APPLICATION_JSON ); return new HttpEntity <> ( request , headers ); } Map < String , String > request = Collections . singletonMap ( \"query\" , graphQLQueryRequest . serialize ()); // Invoke REST call, and get the \"ticks\" from data. JsonNode node = dgsRestTemplate . exchange ( URL , HttpMethod . POST , httpEntity ( mapper . writeValueAsString ( request )), new ParameterizedTypeReference < JsonNode > () { }). getBody (). get ( \"data\" ). get ( \"ticks\" ); //Convert to the response type TicksConnection ticks = mapper . convertValue ( node , TicksConnection . class );","title":"Sending a Query"},{"location":"getting-started/","text":"Create a new Spring Boot application The DGS framework is based on Spring Boot, so get started by creating a new Spring Boot application if you don't have one already. The Spring Initializr is an easy way to do so. You can use either Gradle or Maven, Java 8 or newer or use Kotlin. We do recommend Gradle because we have a really cool code generation plugin for it! The only Spring dependency needed is Spring Web. Open the project in an IDE (Intellij recommended). Adding the [DGS] Framework Dependency Add the com.netflix.graphql.dgs:graphql-dgs-spring-boot-starter dependency to your Gradle or Maven configuration. Gradle dependencies { api \"com.netflix.graphql.dgs:graphql-dgs-spring-boot-starter:latest.release\" } Gradle Kotlin dependencies { api ( \"com.netflix.graphql.dgs:graphql-dgs-spring-boot-starter:3.0.10\" ) } Maven <dependency> <groupId> com.netflix.graphql.dgs </groupId> <artifactId> graphql-dgs-spring-boot-starter </artifactId> <!-- Make sure to set the latest framework version! --> <version> ${dgs.framework.version} </version> </dependency> Creating a Schema The DGS framework is designed for schema first development. The framework picks up any schema files in the src/main/resources/schema folder. Create a schema file in: src/main/resources/schema/schema.graphqls . type Query { shows(titleFilter: String): [Show] } type Show { title: String releaseYear: Int } This schema allows querying for a list of shows, optionally filtering by title. Implement a Data Fetcher Data fetchers are responsible for returning data for a query. Create two new classes example.ShowsDataFetcher and Show and add the following code. Java @DgsComponent public class ShowsDatafetcher { private final List < Show > shows = List . of ( new Show ( \"Stranger Things\" , 2016 ), new Show ( \"Ozark\" , 2017 ), new Show ( \"The Crown\" , 2016 ), new Show ( \"Dead to Me\" , 2019 ), new Show ( \"Orange is the New Black\" , 2013 ) ); @DgsData ( parentType = \"Query\" , field = \"shows\" ) public List < Show > shows ( @InputArgument ( \"titleFilter\" ) String titleFilter ) { if ( titleFilter == null ) { return shows ; } return shows . stream (). filter ( s -> s . getTitle (). contains ( titleFilter )). collect ( Collectors . toList ()); } } public class Show { private final String title ; private final Integer releaseYear ; public Show ( String title , Integer releaseYear ) { this . title = title ; this . releaseYear = releaseYear ; } public String getTitle () { return title ; } public Integer getReleaseYear () { return releaseYear ; } } Kotlin @DgsComponent class ShowsDataFetcher { private val shows = listOf ( Show ( \"Stranger Things\" , 2016 ), Show ( \"Ozark\" , 2017 ), Show ( \"The Crown\" , 2016 ), Show ( \"Dead to Me\" , 2019 ), Show ( \"Orange is the New Black\" , 2013 )) @DgsData ( parentType = \"Query\" , field = \"shows\" ) fun shows ( @InputArgument ( \"titleFilter\" ) titleFilter : String ?): List < Show > { return if ( titleFilter != null ) { shows . filter { it . title . contains ( titleFilter ) } } else { shows } } data class Show ( val title : String , val releaseYear : Int ) } That's all the code needed, the application is ready to be tested! Test the app with GraphiQL Start the application and open a browser to http://localhost:8080/graphiql. GraphiQL is a query editor that comes out of the box with the DGS framework. Write the following query and tests the result. { shows { title releaseYear } } Note that unlike with REST, you have to specifically list which fields you want to get returned from your query. This is where a lot of the power from GraphQL comes from, but a surprise to many developers new to GraphQL. The GraphiQL editor is really just a UI that uses the /graphql endpoint of your service. You could now connect a UI to your backend as well, for example using React and the Apollo Client . Next steps Now that you have a first GraphQL service running, we recommend improving this further by doing the following: Use the Gradle CodeGen plugin - this will generate the data types for you. Write query tests in JUnit","title":"Getting Started"},{"location":"getting-started/#create-a-new-spring-boot-application","text":"The DGS framework is based on Spring Boot, so get started by creating a new Spring Boot application if you don't have one already. The Spring Initializr is an easy way to do so. You can use either Gradle or Maven, Java 8 or newer or use Kotlin. We do recommend Gradle because we have a really cool code generation plugin for it! The only Spring dependency needed is Spring Web. Open the project in an IDE (Intellij recommended).","title":"Create a new Spring Boot application"},{"location":"getting-started/#adding-the-dgs-framework-dependency","text":"Add the com.netflix.graphql.dgs:graphql-dgs-spring-boot-starter dependency to your Gradle or Maven configuration. Gradle dependencies { api \"com.netflix.graphql.dgs:graphql-dgs-spring-boot-starter:latest.release\" } Gradle Kotlin dependencies { api ( \"com.netflix.graphql.dgs:graphql-dgs-spring-boot-starter:3.0.10\" ) } Maven <dependency> <groupId> com.netflix.graphql.dgs </groupId> <artifactId> graphql-dgs-spring-boot-starter </artifactId> <!-- Make sure to set the latest framework version! --> <version> ${dgs.framework.version} </version> </dependency>","title":"Adding the [DGS] Framework Dependency"},{"location":"getting-started/#creating-a-schema","text":"The DGS framework is designed for schema first development. The framework picks up any schema files in the src/main/resources/schema folder. Create a schema file in: src/main/resources/schema/schema.graphqls . type Query { shows(titleFilter: String): [Show] } type Show { title: String releaseYear: Int } This schema allows querying for a list of shows, optionally filtering by title.","title":"Creating a Schema"},{"location":"getting-started/#implement-a-data-fetcher","text":"Data fetchers are responsible for returning data for a query. Create two new classes example.ShowsDataFetcher and Show and add the following code. Java @DgsComponent public class ShowsDatafetcher { private final List < Show > shows = List . of ( new Show ( \"Stranger Things\" , 2016 ), new Show ( \"Ozark\" , 2017 ), new Show ( \"The Crown\" , 2016 ), new Show ( \"Dead to Me\" , 2019 ), new Show ( \"Orange is the New Black\" , 2013 ) ); @DgsData ( parentType = \"Query\" , field = \"shows\" ) public List < Show > shows ( @InputArgument ( \"titleFilter\" ) String titleFilter ) { if ( titleFilter == null ) { return shows ; } return shows . stream (). filter ( s -> s . getTitle (). contains ( titleFilter )). collect ( Collectors . toList ()); } } public class Show { private final String title ; private final Integer releaseYear ; public Show ( String title , Integer releaseYear ) { this . title = title ; this . releaseYear = releaseYear ; } public String getTitle () { return title ; } public Integer getReleaseYear () { return releaseYear ; } } Kotlin @DgsComponent class ShowsDataFetcher { private val shows = listOf ( Show ( \"Stranger Things\" , 2016 ), Show ( \"Ozark\" , 2017 ), Show ( \"The Crown\" , 2016 ), Show ( \"Dead to Me\" , 2019 ), Show ( \"Orange is the New Black\" , 2013 )) @DgsData ( parentType = \"Query\" , field = \"shows\" ) fun shows ( @InputArgument ( \"titleFilter\" ) titleFilter : String ?): List < Show > { return if ( titleFilter != null ) { shows . filter { it . title . contains ( titleFilter ) } } else { shows } } data class Show ( val title : String , val releaseYear : Int ) } That's all the code needed, the application is ready to be tested!","title":"Implement a Data Fetcher"},{"location":"getting-started/#test-the-app-with-graphiql","text":"Start the application and open a browser to http://localhost:8080/graphiql. GraphiQL is a query editor that comes out of the box with the DGS framework. Write the following query and tests the result. { shows { title releaseYear } } Note that unlike with REST, you have to specifically list which fields you want to get returned from your query. This is where a lot of the power from GraphQL comes from, but a surprise to many developers new to GraphQL. The GraphiQL editor is really just a UI that uses the /graphql endpoint of your service. You could now connect a UI to your backend as well, for example using React and the Apollo Client .","title":"Test the app with GraphiQL"},{"location":"getting-started/#next-steps","text":"Now that you have a first GraphQL service running, we recommend improving this further by doing the following: Use the Gradle CodeGen plugin - this will generate the data types for you. Write query tests in JUnit","title":"Next steps"},{"location":"mutations/","text":"The DGS framework supports Mutations with the same constructs as data fetchers, using the @DgsData annotation. The following is a simple example of a mutation: type Mutation { addRating(title: String, stars: Int):Rating } type Rating { avgStars: Float } @DgsComponent public class RatingMutation { @DgsData ( parentType = \"Mutation\" , field = \"addRating\" ) public Rating addRating ( DataFetchingEnvironment dataFetchingEnvironment ) { int stars = dataFetchingEnvironment . getArgument ( \"stars\" ); if ( stars < 1 ) { throw new IllegalArgumentException ( \"Stars must be 1-5\" ); } String title = dataFetchingEnvironment . getArgument ( \"title\" ); System . out . println ( \"Rated \" + title + \" with \" + stars + \" stars\" ) ; return new Rating ( stars ); } } Note that the code above retrieves the input data for the Mutation by calling the DataFetchingEnvironment.getArgument method, just as data fetchers do for their arguments. Input Types In the example above the input was two standard scalar types. You can also use complex types, and you should define these as input types in your schema. An input type is almost the same as a type in GraphQL, but with some extra rules . According to the GraphQL specification an input type should always be passed to the data fetcher as a Map . This means the DataFetchingEnvironment.getArgument for an input type is a Map , and not the Java/Kotlin representation that you might have. The framework has a convenience mechanism around this, which will be discussed next. Let's first look at an example that uses DataFetchingEnvironment directly. type Mutation { addRating(input: RatingInput):Rating } input RatingInput { title: String, stars: Int } type Rating { avgStars: Float } @DgsComponent public class RatingMutation { @DgsData ( parentType = \"Mutation\" , field = \"addRating\" ) public Rating addRating ( DataFetchingEnvironment dataFetchingEnvironment ) { Map < String , Object > input = dataFetchingEnvironment . getArgument ( \"input\" ); RatingInput ratingInput = new ObjectMapper (). convertValue ( input , RatingInput . class ); System . out . println ( \"Rated \" + ratingInput . getTitle () + \" with \" + ratingInput . getStars () + \" stars\" ) ; return new Rating ( ratingInput . getStars ()); } } class RatingInput { private String title ; private int stars ; public String getTitle () { return title ; } public void setTitle ( String title ) { this . title = title ; } public int getStars () { return stars ; } public void setStars ( int stars ) { this . stars = stars ; } } Input arguments as data fetcher method parameters The framework makes it easier to get input arguments. You can specify arguments as method parameters of a data fetcher. @DgsComponent public class RatingMutation { @DgsData ( parentType = \"Mutation\" , field = \"addRating\" ) public Rating addRating ( @InputArgument ( \"input\" ) RatingInput ratingInput ) { //No need for custom parsing anymore! System . out . println ( \"Rated \" + ratingInput . getTitle () + \" with \" + ratingInput . getStars () + \" stars\" ) ; return new Rating ( ratingInput . getStars ()); } } The @InputArgument annotation is important to specify the name of the input argument, because arguments can be specified in any order. If no annotation is present, the framework tries to use the parameter name, but this is only possible if the code is compiled with specific compiler settings . Input type parameters can be combined with a DataFetchingEnvironment parameter. @DgsComponent public class RatingMutation { @DgsData ( parentType = \"Mutation\" , field = \"addRating\" ) public Rating addRating ( @InputArgument ( \"input\" ) RatingInput ratingInput , DataFetchingEnvironment dfe ) { //No need for custom parsing anymore! System . out . println ( \"Rated \" + ratingInput . getTitle () + \" with \" + ratingInput . getStars () + \" stars\" ) ; System . out . println ( \"DataFetchingEnvironment: \" + dfe . getArgument ( ratingInput )); return new Rating ( ratingInput . getStars ()); } } Kotlin data types In Kotlin, you can use Data Classes to represent input types. However, make sure its fields are either var or add a @JsonProperty to each constructor argument, and use jacksonObjectMapper() to create a Kotlin-compatible Jackson mapper. data class RatingInput ( var title : String , var stars : Int )","title":"Mutations"},{"location":"mutations/#input-types","text":"In the example above the input was two standard scalar types. You can also use complex types, and you should define these as input types in your schema. An input type is almost the same as a type in GraphQL, but with some extra rules . According to the GraphQL specification an input type should always be passed to the data fetcher as a Map . This means the DataFetchingEnvironment.getArgument for an input type is a Map , and not the Java/Kotlin representation that you might have. The framework has a convenience mechanism around this, which will be discussed next. Let's first look at an example that uses DataFetchingEnvironment directly. type Mutation { addRating(input: RatingInput):Rating } input RatingInput { title: String, stars: Int } type Rating { avgStars: Float } @DgsComponent public class RatingMutation { @DgsData ( parentType = \"Mutation\" , field = \"addRating\" ) public Rating addRating ( DataFetchingEnvironment dataFetchingEnvironment ) { Map < String , Object > input = dataFetchingEnvironment . getArgument ( \"input\" ); RatingInput ratingInput = new ObjectMapper (). convertValue ( input , RatingInput . class ); System . out . println ( \"Rated \" + ratingInput . getTitle () + \" with \" + ratingInput . getStars () + \" stars\" ) ; return new Rating ( ratingInput . getStars ()); } } class RatingInput { private String title ; private int stars ; public String getTitle () { return title ; } public void setTitle ( String title ) { this . title = title ; } public int getStars () { return stars ; } public void setStars ( int stars ) { this . stars = stars ; } }","title":"Input Types"},{"location":"mutations/#input-arguments-as-data-fetcher-method-parameters","text":"The framework makes it easier to get input arguments. You can specify arguments as method parameters of a data fetcher. @DgsComponent public class RatingMutation { @DgsData ( parentType = \"Mutation\" , field = \"addRating\" ) public Rating addRating ( @InputArgument ( \"input\" ) RatingInput ratingInput ) { //No need for custom parsing anymore! System . out . println ( \"Rated \" + ratingInput . getTitle () + \" with \" + ratingInput . getStars () + \" stars\" ) ; return new Rating ( ratingInput . getStars ()); } } The @InputArgument annotation is important to specify the name of the input argument, because arguments can be specified in any order. If no annotation is present, the framework tries to use the parameter name, but this is only possible if the code is compiled with specific compiler settings . Input type parameters can be combined with a DataFetchingEnvironment parameter. @DgsComponent public class RatingMutation { @DgsData ( parentType = \"Mutation\" , field = \"addRating\" ) public Rating addRating ( @InputArgument ( \"input\" ) RatingInput ratingInput , DataFetchingEnvironment dfe ) { //No need for custom parsing anymore! System . out . println ( \"Rated \" + ratingInput . getTitle () + \" with \" + ratingInput . getStars () + \" stars\" ) ; System . out . println ( \"DataFetchingEnvironment: \" + dfe . getArgument ( ratingInput )); return new Rating ( ratingInput . getStars ()); } }","title":"Input arguments as data fetcher method parameters"},{"location":"mutations/#kotlin-data-types","text":"In Kotlin, you can use Data Classes to represent input types. However, make sure its fields are either var or add a @JsonProperty to each constructor argument, and use jacksonObjectMapper() to create a Kotlin-compatible Jackson mapper. data class RatingInput ( var title : String , var stars : Int )","title":"Kotlin data types"},{"location":"query-execution-testing/","text":"The DGS framework allows you to write lightweight tests that partially bootstrap the framework, just enough to run queries. Example Before writing tests, make sure that JUnit is enabled. If you created a project with Spring Initializr this configuration should already be there. Gradle dependencies { testImplementation 'org.springframework.boot:spring-boot-starter-test' } test { useJUnitPlatform () } Gradle Kotlin tasks . withType < Test > { useJUnitPlatform () } Maven <dependency> <groupId> org.springframework.boot </groupId> <artifactId> spring-boot-starter-test </artifactId> <scope> test </scope> </dependency> Create a test class with the following contents to test the ShowsDatafetcher from the getting started example. Java import com.netflix.graphql.dgs.DgsQueryExecutor ; import com.netflix.graphql.dgs.autoconfig.DgsAutoConfiguration ; import org.junit.jupiter.api.Test ; import org.springframework.beans.factory.annotation.Autowired ; import org.springframework.boot.test.context.SpringBootTest ; import java.util.List ; import static org.assertj.core.api.Assertions.assertThat ; @SpringBootTest ( classes = { DgsAutoConfiguration . class , ShowsDatafetcher . class }) class ShowsDatafetcherTest { @Autowired DgsQueryExecutor dgsQueryExecutor ; @Test void shows () { List < String > titles = dgsQueryExecutor . executeAndExtractJsonPath ( \" { shows { title releaseYear }}\" , \"data.shows[*].title\" ); assertThat ( titles ). contains ( \"Ozark\" ); } } Kotlin import com.netflix.graphql.dgs.DgsQueryExecutor import com.netflix.graphql.dgs.autoconfig.DgsAutoConfiguration import org.assertj.core.api.Assertions.assertThat import org.junit.jupiter.api.Test import org.springframework.beans.factory.annotation.Autowired import org.springframework.boot.test.context.SpringBootTest @SpringBootTest ( classes = [ DgsAutoConfiguration :: class , ShowsDataFetcher :: class ]) class ShowsDataFetcherTest { @Autowired lateinit var dgsQueryExecutor : DgsQueryExecutor @Test fun shows () { val titles : List < String > = dgsQueryExecutor . executeAndExtractJsonPath ( \"\"\" { shows { title releaseYear } } \"\"\" . trimIndent (), \"data.shows[*].title\" ) assertThat ( titles ). contains ( \"Ozark\" ) } } The @SpringBootTest annotation makes this a Spring test. If you do not specify classes explicitly, Spring will start all components on the classpath. For a small application this is fine, but for applications with components that are \"expensive\" to start we can speed up the test by only adding the classes we need for the test. In this case we need to include the DGS framework itself using the DgsAutoConfiguration class, and the ShowsDatafetcher . To execute queries, inject DgsQueryExecutor in the test. This interface has several methods to execute a query and get back the result. It executes the exact same code as a query on the /graphql endpoint would, but you won\u2019t have to deal with HTTP in your tests. The DgsQueryExecutor methods accept JSON paths, so that the methods can easily extract just the data from the response that you\u2019re interested in. The DgsQueryExecutor also includes methods (e.g. executeAndExtractJsonPathAsObject ) to deserialize the result to a Java class, which uses Jackson under the hood. The JSON paths are supported by the open source JsonPath library . Write a few more tests, for example to verify the behavior with using the titleFilter of ShowsDatafetcher . You can run the tests from the IDE, or from Gradle/Maven, just like any JUnit test. Building GraphQL Queries for Tests In the examples shown previously, we handcrafted the query string. This is simple enough for queries that are small and straightforward. However, constructing longer query strings can be tedious, specially in Java without support for multi-line Strings. For this, we can use the GraphQLQueryRequest to build the graphql request in combination with the code generation plugin to generate the classes needed to use the request builder. This provides a convenient type-safe way to build your queries. To set up code generation to generate the required classes to use for building your queries, follow the instructions here . Now we can write a test that uses GraphQLQueryRequest to build the query and extract the response using GraphQLResponse . Java @Test public void showsWithQueryApi () { GraphQLQueryRequest graphQLQueryRequest = new GraphQLQueryRequest ( new ShowsGraphQLQuery . Builder (). titleFilter ( \"Oz\" ). build (), new ShowsProjectionRoot (). title () ); List < String > titles = dgsQueryExecutor . executeAndExtractJsonPath ( graphQLQueryRequest . serialize (), \"data.shows[*].title\" ); assertThat ( titles ). containsExactly ( \"Ozark\" ); } Kotlin @Test fun showsWithQueryApi () { val graphQLQueryRequest = GraphQLQueryRequest ( ShowsGraphQLQuery . Builder () . titleFilter ( \"Oz\" ) . build (), ShowsProjectionRoot (). title ()) val titles = dgsQueryExecutor . executeAndExtractJsonPath < List < String >>( graphQLQueryRequest . serialize (), \"data.shows[*].title\" ) assertThat ( titles ). containsExactly ( \"Ozark\" ) } The GraphQLQueryRequest is available as part of the graphql-client module and is used to build the query string, and wrap the response respectively. You can also refer to the GraphQLClient JavaDoc for more details on the list of supported methods. Mocking External Service Calls in Tests It\u2019s not uncommon for a data fetcher to talk to external systems such as a database or a gRPC service. If it does so within a test, this adds two problems: It adds latency; your tests are going to run slower when they make a lot of external calls. It adds flakiness: Did your code introduce a bug, or did something go wrong in the external system? In many cases it\u2019s better to mock these external services. Spring already has good support for doing so with the @Mockbean annotation, which you can leverage in your DGS tests. Example Let's update the Shows example to load shows from an external data source, instead of just returning a fixed list. For the sake of the example we'll just move the fixed list of shows to a new class that we'll annotate @Service . The data fetcher is updated to use the injected ShowsService . Java public interface ShowsService { List < Show > shows (); } @Service public class ShowsServiceImpl implements ShowsService { @Override public List < Show > shows () { return List . of ( new Show ( \"Stranger Things\" , 2016 ), new Show ( \"Ozark\" , 2017 ), new Show ( \"The Crown\" , 2016 ), new Show ( \"Dead to Me\" , 2019 ), new Show ( \"Orange is the New Black\" , 2013 ) ); } } Kotlin interface ShowsService { fun shows (): List < ShowsDataFetcher . Show > } @Service class BasicShowsService : ShowsService { override fun shows (): List < ShowsDataFetcher . Show > { return listOf ( ShowsDataFetcher . Show ( \"Stranger Things\" , 2016 ), ShowsDataFetcher . Show ( \"Ozark\" , 2017 ), ShowsDataFetcher . Show ( \"The Crown\" , 2016 ), ShowsDataFetcher . Show ( \"Dead to Me\" , 2019 ), ShowsDataFetcher . Show ( \"Orange is the New Black\" , 2013 ) ) } } @DgsComponent class ShowsDataFetcher { @Autowired lateinit var showsService : ShowsService @DgsData ( parentType = \"Query\" , field = \"shows\" ) fun shows ( @InputArgument ( \"titleFilter\" ) titleFilter : String ?): List < Show > { return if ( titleFilter != null ) { showsService . shows (). filter { it . title . contains ( titleFilter ) } } else { showsService . shows () } } } For the sake of the example the shows are still in-memory, imagine that the service would actually call out to an external data store. Let's try to mock this service in the test! Java @SpringBootTest ( classes = { DgsAutoConfiguration . class , ShowsDataFetcher . class }) public class ShowsDataFetcherTests { @Autowired DgsQueryExecutor dgsQueryExecutor ; @MockBean ShowsService showsService ; @BeforeEach public void before () { Mockito . when ( showsService . shows ()). thenAnswer ( invocation -> List . of ( new Show ( \"mock title\" , 2020 ))); } @Test public void showsWithQueryApi () { GraphQLQueryRequest graphQLQueryRequest = new GraphQLQueryRequest ( new ShowsGraphQLQuery . Builder (). build (), new ShowsProjectionRoot (). title () ); List < String > titles = dgsQueryExecutor . executeAndExtractJsonPath ( graphQLQueryRequest . serialize (), \"data.shows[*].title\" ); assertThat ( titles ). containsExactly ( \"mock title\" ); } } Kotlin @SpringBootTest ( classes = [ DgsAutoConfiguration :: class , ShowsDataFetcher :: class ]) class ShowsDataFetcherTest { @Autowired lateinit var dgsQueryExecutor : DgsQueryExecutor @MockBean lateinit var showsService : ShowsService @BeforeEach fun before () { Mockito . `when` ( showsService . shows ()). thenAnswer { listOf ( ShowsDataFetcher . Show ( \"mock title\" , 2020 )) } } @Test fun shows () { val titles : List < String > = dgsQueryExecutor . executeAndExtractJsonPath ( \"\"\" { shows { title releaseYear } } \"\"\" . trimIndent (), \"data.shows[*].title\" ) assertThat ( titles ). contains ( \"mock title\" ) } } Testing Exceptions The tests you wrote so far are mostly happy paths. Failure scenarios are also easy to test. We use the mocked example from above to force an exception. Java @Test void showsWithException () { Mockito . when ( showsService . shows ()). thenThrow ( new RuntimeException ( \"nothing to see here\" )); ExecutionResult result = dgsQueryExecutor . execute ( \" { shows { title releaseYear }}\" ); assertThat ( result . getErrors ()). isNotEmpty (); assertThat ( result . getErrors (). get ( 0 ). getMessage ()). isEqualTo ( \"java.lang.RuntimeException: nothing to see here\" ); } Kotlin @Test fun showsWithException () { Mockito . `when` ( showsService . shows ()). thenThrow ( RuntimeException ( \"nothing to see here\" )) val result = dgsQueryExecutor . execute ( \"\"\" { shows { title releaseYear } } \"\"\" . trimIndent ()) assertThat ( result . errors ). isNotEmpty assertThat ( result . errors [ 0 ]. message ). isEqualTo ( \"java.lang.RuntimeException: nothing to see here\" ) } When an error happens while executing the query, the errors are wrapped in a QueryException . This allows you to easily inspect the error. The message of the QueryException is the concatenation of all the errors. The getErrors() method gives access to the individual errors for further inspection.","title":"Testing"},{"location":"query-execution-testing/#example","text":"Before writing tests, make sure that JUnit is enabled. If you created a project with Spring Initializr this configuration should already be there. Gradle dependencies { testImplementation 'org.springframework.boot:spring-boot-starter-test' } test { useJUnitPlatform () } Gradle Kotlin tasks . withType < Test > { useJUnitPlatform () } Maven <dependency> <groupId> org.springframework.boot </groupId> <artifactId> spring-boot-starter-test </artifactId> <scope> test </scope> </dependency> Create a test class with the following contents to test the ShowsDatafetcher from the getting started example. Java import com.netflix.graphql.dgs.DgsQueryExecutor ; import com.netflix.graphql.dgs.autoconfig.DgsAutoConfiguration ; import org.junit.jupiter.api.Test ; import org.springframework.beans.factory.annotation.Autowired ; import org.springframework.boot.test.context.SpringBootTest ; import java.util.List ; import static org.assertj.core.api.Assertions.assertThat ; @SpringBootTest ( classes = { DgsAutoConfiguration . class , ShowsDatafetcher . class }) class ShowsDatafetcherTest { @Autowired DgsQueryExecutor dgsQueryExecutor ; @Test void shows () { List < String > titles = dgsQueryExecutor . executeAndExtractJsonPath ( \" { shows { title releaseYear }}\" , \"data.shows[*].title\" ); assertThat ( titles ). contains ( \"Ozark\" ); } } Kotlin import com.netflix.graphql.dgs.DgsQueryExecutor import com.netflix.graphql.dgs.autoconfig.DgsAutoConfiguration import org.assertj.core.api.Assertions.assertThat import org.junit.jupiter.api.Test import org.springframework.beans.factory.annotation.Autowired import org.springframework.boot.test.context.SpringBootTest @SpringBootTest ( classes = [ DgsAutoConfiguration :: class , ShowsDataFetcher :: class ]) class ShowsDataFetcherTest { @Autowired lateinit var dgsQueryExecutor : DgsQueryExecutor @Test fun shows () { val titles : List < String > = dgsQueryExecutor . executeAndExtractJsonPath ( \"\"\" { shows { title releaseYear } } \"\"\" . trimIndent (), \"data.shows[*].title\" ) assertThat ( titles ). contains ( \"Ozark\" ) } } The @SpringBootTest annotation makes this a Spring test. If you do not specify classes explicitly, Spring will start all components on the classpath. For a small application this is fine, but for applications with components that are \"expensive\" to start we can speed up the test by only adding the classes we need for the test. In this case we need to include the DGS framework itself using the DgsAutoConfiguration class, and the ShowsDatafetcher . To execute queries, inject DgsQueryExecutor in the test. This interface has several methods to execute a query and get back the result. It executes the exact same code as a query on the /graphql endpoint would, but you won\u2019t have to deal with HTTP in your tests. The DgsQueryExecutor methods accept JSON paths, so that the methods can easily extract just the data from the response that you\u2019re interested in. The DgsQueryExecutor also includes methods (e.g. executeAndExtractJsonPathAsObject ) to deserialize the result to a Java class, which uses Jackson under the hood. The JSON paths are supported by the open source JsonPath library . Write a few more tests, for example to verify the behavior with using the titleFilter of ShowsDatafetcher . You can run the tests from the IDE, or from Gradle/Maven, just like any JUnit test.","title":"Example"},{"location":"query-execution-testing/#building-graphql-queries-for-tests","text":"In the examples shown previously, we handcrafted the query string. This is simple enough for queries that are small and straightforward. However, constructing longer query strings can be tedious, specially in Java without support for multi-line Strings. For this, we can use the GraphQLQueryRequest to build the graphql request in combination with the code generation plugin to generate the classes needed to use the request builder. This provides a convenient type-safe way to build your queries. To set up code generation to generate the required classes to use for building your queries, follow the instructions here . Now we can write a test that uses GraphQLQueryRequest to build the query and extract the response using GraphQLResponse . Java @Test public void showsWithQueryApi () { GraphQLQueryRequest graphQLQueryRequest = new GraphQLQueryRequest ( new ShowsGraphQLQuery . Builder (). titleFilter ( \"Oz\" ). build (), new ShowsProjectionRoot (). title () ); List < String > titles = dgsQueryExecutor . executeAndExtractJsonPath ( graphQLQueryRequest . serialize (), \"data.shows[*].title\" ); assertThat ( titles ). containsExactly ( \"Ozark\" ); } Kotlin @Test fun showsWithQueryApi () { val graphQLQueryRequest = GraphQLQueryRequest ( ShowsGraphQLQuery . Builder () . titleFilter ( \"Oz\" ) . build (), ShowsProjectionRoot (). title ()) val titles = dgsQueryExecutor . executeAndExtractJsonPath < List < String >>( graphQLQueryRequest . serialize (), \"data.shows[*].title\" ) assertThat ( titles ). containsExactly ( \"Ozark\" ) } The GraphQLQueryRequest is available as part of the graphql-client module and is used to build the query string, and wrap the response respectively. You can also refer to the GraphQLClient JavaDoc for more details on the list of supported methods.","title":"Building GraphQL Queries for Tests"},{"location":"query-execution-testing/#mocking-external-service-calls-in-tests","text":"It\u2019s not uncommon for a data fetcher to talk to external systems such as a database or a gRPC service. If it does so within a test, this adds two problems: It adds latency; your tests are going to run slower when they make a lot of external calls. It adds flakiness: Did your code introduce a bug, or did something go wrong in the external system? In many cases it\u2019s better to mock these external services. Spring already has good support for doing so with the @Mockbean annotation, which you can leverage in your DGS tests.","title":"Mocking External Service Calls in Tests"},{"location":"query-execution-testing/#example_1","text":"Let's update the Shows example to load shows from an external data source, instead of just returning a fixed list. For the sake of the example we'll just move the fixed list of shows to a new class that we'll annotate @Service . The data fetcher is updated to use the injected ShowsService . Java public interface ShowsService { List < Show > shows (); } @Service public class ShowsServiceImpl implements ShowsService { @Override public List < Show > shows () { return List . of ( new Show ( \"Stranger Things\" , 2016 ), new Show ( \"Ozark\" , 2017 ), new Show ( \"The Crown\" , 2016 ), new Show ( \"Dead to Me\" , 2019 ), new Show ( \"Orange is the New Black\" , 2013 ) ); } } Kotlin interface ShowsService { fun shows (): List < ShowsDataFetcher . Show > } @Service class BasicShowsService : ShowsService { override fun shows (): List < ShowsDataFetcher . Show > { return listOf ( ShowsDataFetcher . Show ( \"Stranger Things\" , 2016 ), ShowsDataFetcher . Show ( \"Ozark\" , 2017 ), ShowsDataFetcher . Show ( \"The Crown\" , 2016 ), ShowsDataFetcher . Show ( \"Dead to Me\" , 2019 ), ShowsDataFetcher . Show ( \"Orange is the New Black\" , 2013 ) ) } } @DgsComponent class ShowsDataFetcher { @Autowired lateinit var showsService : ShowsService @DgsData ( parentType = \"Query\" , field = \"shows\" ) fun shows ( @InputArgument ( \"titleFilter\" ) titleFilter : String ?): List < Show > { return if ( titleFilter != null ) { showsService . shows (). filter { it . title . contains ( titleFilter ) } } else { showsService . shows () } } } For the sake of the example the shows are still in-memory, imagine that the service would actually call out to an external data store. Let's try to mock this service in the test! Java @SpringBootTest ( classes = { DgsAutoConfiguration . class , ShowsDataFetcher . class }) public class ShowsDataFetcherTests { @Autowired DgsQueryExecutor dgsQueryExecutor ; @MockBean ShowsService showsService ; @BeforeEach public void before () { Mockito . when ( showsService . shows ()). thenAnswer ( invocation -> List . of ( new Show ( \"mock title\" , 2020 ))); } @Test public void showsWithQueryApi () { GraphQLQueryRequest graphQLQueryRequest = new GraphQLQueryRequest ( new ShowsGraphQLQuery . Builder (). build (), new ShowsProjectionRoot (). title () ); List < String > titles = dgsQueryExecutor . executeAndExtractJsonPath ( graphQLQueryRequest . serialize (), \"data.shows[*].title\" ); assertThat ( titles ). containsExactly ( \"mock title\" ); } } Kotlin @SpringBootTest ( classes = [ DgsAutoConfiguration :: class , ShowsDataFetcher :: class ]) class ShowsDataFetcherTest { @Autowired lateinit var dgsQueryExecutor : DgsQueryExecutor @MockBean lateinit var showsService : ShowsService @BeforeEach fun before () { Mockito . `when` ( showsService . shows ()). thenAnswer { listOf ( ShowsDataFetcher . Show ( \"mock title\" , 2020 )) } } @Test fun shows () { val titles : List < String > = dgsQueryExecutor . executeAndExtractJsonPath ( \"\"\" { shows { title releaseYear } } \"\"\" . trimIndent (), \"data.shows[*].title\" ) assertThat ( titles ). contains ( \"mock title\" ) } }","title":"Example"},{"location":"query-execution-testing/#testing-exceptions","text":"The tests you wrote so far are mostly happy paths. Failure scenarios are also easy to test. We use the mocked example from above to force an exception. Java @Test void showsWithException () { Mockito . when ( showsService . shows ()). thenThrow ( new RuntimeException ( \"nothing to see here\" )); ExecutionResult result = dgsQueryExecutor . execute ( \" { shows { title releaseYear }}\" ); assertThat ( result . getErrors ()). isNotEmpty (); assertThat ( result . getErrors (). get ( 0 ). getMessage ()). isEqualTo ( \"java.lang.RuntimeException: nothing to see here\" ); } Kotlin @Test fun showsWithException () { Mockito . `when` ( showsService . shows ()). thenThrow ( RuntimeException ( \"nothing to see here\" )) val result = dgsQueryExecutor . execute ( \"\"\" { shows { title releaseYear } } \"\"\" . trimIndent ()) assertThat ( result . errors ). isNotEmpty assertThat ( result . errors [ 0 ]. message ). isEqualTo ( \"java.lang.RuntimeException: nothing to see here\" ) } When an error happens while executing the query, the errors are wrapped in a QueryException . This allows you to easily inspect the error. The message of the QueryException is the concatenation of all the errors. The getErrors() method gives access to the individual errors for further inspection.","title":"Testing Exceptions"},{"location":"scalars/","text":"It is easy to add a custom scalar type in the [DGS] framework: Create a class that implements the graphql.schema.Coercing interface and annotate it with the @DgsScalar annotation. Also make sure the scalar type is defined in your [GraphQL] schema! For example, this is a simple LocalDateTime implementation: @DgsScalar ( name = \"DateTime\" ) public class DateTimeScalar implements Coercing < LocalDateTime , String > { @Override public String serialize ( Object dataFetcherResult ) throws CoercingSerializeException { if ( dataFetcherResult instanceof LocalDateTime ) { return (( LocalDateTime ) dataFetcherResult ). format ( DateTimeFormatter . ISO_DATE_TIME ); } else { throw new CoercingSerializeException ( \"Not a valid DateTime\" ); } } @Override public LocalDateTime parseValue ( Object input ) throws CoercingParseValueException { return LocalDateTime . parse ( input . toString (), DateTimeFormatter . ISO_DATE_TIME ); } @Override public LocalDateTime parseLiteral ( Object input ) throws CoercingParseLiteralException { if ( input instanceof StringValue ) { return LocalDateTime . parse ((( StringValue ) input ). getValue (), DateTimeFormatter . ISO_DATE_TIME ); } throw new CoercingParseLiteralException ( \"Value is not a valid ISO date time\" ); } } Schema: scalar DateTime","title":"Adding Custom Scalars"},{"location":"advanced/custom-datafetcher-context/","text":"Each data fetcher in [GraphQL] Java has a context. A data fetcher gets access to its context by calling DataFetchingEnvironment.getContext() . This is a common mechanism to pass request context to data fetchers and data loaders. The [DGS] framework has its own DgsContext implementation, which is used for log instrumentation among other things. It is designed in such a way that you can extend it with your own custom context. To create a custom context, implement a Spring bean of type DgsCustomContextBuilder . Write the build() method so that it creates an instance of the type that represents your custom context object: @Component public class MyContextBuilder implements DgsCustomContextBuilder < MyContext > { @Override public MyContext build () { return new MyContext (); } } public class MyContext { private final String customState = \"Custom state!\" ; public String getCustomState () { return customState ; } } A data fetcher can now retrieve the context by calling the getCustomContext() method: @DgsData ( parentType = \"Query\" , field = \"withContext\" ) public String withContext ( DataFetchingEnvironment dfe ) { MyContext customContext = DgsContext . getCustomContext ( dfe ); return customContext . getCustomState (); } Similarly it can be used in a DataLoader. @DgsDataLoader ( name = \"exampleLoaderWithContext\" ) public class ExampleLoaderWithContext implements BatchLoaderWithContext < String , String > { @Override public CompletionStage < List < String >> load ( List < String > keys , BatchLoaderEnvironment environment ) { MyContext context = DgsContext . getCustomContext ( environment ); return CompletableFuture . supplyAsync (() -> keys . stream (). map ( key -> context . getCustomState () + \" \" + key ). collect ( Collectors . toList ())); } }","title":"Data Fetching Context"},{"location":"advanced/federated-testing/","text":"Federation allows you to extend or reference existing types in a graph. Your DGS fulfills a part of the query based on the schema that is owned by your DGS, while the gateway is responsible for fetching data from other DGSs. Testing Federated Queries without the Gateway You can test federated queries for your DGS in isolation by replicating the format of the query that the gateway would send to your DGS. This does not involve the gateway, and thus the parts of the query response that your DGS is not responsible for will not be hydrated. This technique is useful if you want to verify that your DGS is able to return the appropriate data, in response to a federated query. Let's look at an example of a schema that extends the Movie type that is already defined by another DGS. type Movie @key(fields: \"movieId\") @extends { movieId: Int @external script: MovieScript } type MovieScript { title: String director: String actors: [Actor] } type Actor { name: String gender: String age: Int } Now you want to verify that your DGS is able to fulfill the Movie query by hydrating the script field based on the movieId field. Normally, the gateway would send an _entities query in the following format: query ( $representations : [ _Any !]! ) { _entities ( representations : $representations ) { ... on Movie { movieId script { title } }}} The representations input is a variable map containing the __typename field set to Movie and movieId set to a value, e.g., 12345 . You can now set up a Query Executor test by either manually constructing the query, or you can generate the federated query using the Entities Query Builder API available through client code generation . Here is an example of a test that uses a manually constructed _entities query for Movie : @Test void federatedMovieQuery () throws IOException { String query = \"query ($representations: [_Any!]!) {\" + \"_entities(representations: $representations) {\" + \"... on Movie {\" + \"movieId \" + \"script { title }\" + \"}}}\" ; Map < String , Object > variables = new HashMap <> (); Map < String , Object > representation = new HashMap <> (); representation . put ( \"__typename\" , \"Movie\" ); representation . put ( \"movieId\" , 1 ); variables . put ( \"representations\" , List . of ( representation )); DocumentContext context = queryExecutor . executeAndGetDocumentContext ( query , variables ); GraphQLResponse response = new GraphQLResponse ( context . jsonString ()); Movie movie = response . extractValueAsObject ( \"data._entities[0]\" , Movie . class ); assertThat ( movie . getScript (). getTitle ()). isEqualTo ( \"Top Secret\" ); } Using the Entities Query Builder API Alternatively, you can generate the federated query by using EntitiesGraphQLQuery to build the graphql request in combination with the code generation plugin to generate the classes needed to use the request builder. This provides a convenient type-safe way to build your queries. To set up code generation to generate the required classes to use for building your queries, follow the instructions here . You will also need to add com.netflix.graphql.dgs:graphql-dgs-client:latest.release dependency to build.gradle. Now we can write a test that uses EntitiesGraphQLQuery along with GraphQLQueryRequest and EntitiesProjectionRoot to build the query. Finally, you can also extract the response using GraphQLResponse . This set up is shown here: @Test void federatedMovieQueryAPI () throws IOException { // constructs the _entities query with variable $representations containing a // movie representation that represents { __typename: \"Movie\" movieId: 12345 } EntitiesGraphQLQuery entitiesQuery = new EntitiesGraphQLQuery . Builder () . addRepresentationAsVariable ( MovieRepresentation . newBuilder (). movieId ( 1122 ). build () ) . build (); // sets up the query and the field selection set using the EntitiesProjectionRoot GraphQLQueryRequest request = new GraphQLQueryRequest ( entitiesQuery , new EntitiesProjectionRoot (). onMovie (). movieId (). script (). title ()); String query = request . serialize (); // pass in the constructed _entities query with the variable map containing representations DocumentContext context = queryExecutor . executeAndGetDocumentContext ( query , entitiesQuery . getVariables ()); GraphQLResponse response = new GraphQLResponse ( context . jsonString ()); Movie movie = response . extractValueAsObject ( \"data._entities[0]\" , Movie . class ); assertThat ( movie . getScript (). getTitle ()). isEqualTo ( \"Top Secret\" ); } Check out this video for a demo on how to configure and write the above test.","title":"Federated Testing"},{"location":"advanced/federated-testing/#testing-federated-queries-without-the-gateway","text":"You can test federated queries for your DGS in isolation by replicating the format of the query that the gateway would send to your DGS. This does not involve the gateway, and thus the parts of the query response that your DGS is not responsible for will not be hydrated. This technique is useful if you want to verify that your DGS is able to return the appropriate data, in response to a federated query. Let's look at an example of a schema that extends the Movie type that is already defined by another DGS. type Movie @key(fields: \"movieId\") @extends { movieId: Int @external script: MovieScript } type MovieScript { title: String director: String actors: [Actor] } type Actor { name: String gender: String age: Int } Now you want to verify that your DGS is able to fulfill the Movie query by hydrating the script field based on the movieId field. Normally, the gateway would send an _entities query in the following format: query ( $representations : [ _Any !]! ) { _entities ( representations : $representations ) { ... on Movie { movieId script { title } }}} The representations input is a variable map containing the __typename field set to Movie and movieId set to a value, e.g., 12345 . You can now set up a Query Executor test by either manually constructing the query, or you can generate the federated query using the Entities Query Builder API available through client code generation . Here is an example of a test that uses a manually constructed _entities query for Movie : @Test void federatedMovieQuery () throws IOException { String query = \"query ($representations: [_Any!]!) {\" + \"_entities(representations: $representations) {\" + \"... on Movie {\" + \"movieId \" + \"script { title }\" + \"}}}\" ; Map < String , Object > variables = new HashMap <> (); Map < String , Object > representation = new HashMap <> (); representation . put ( \"__typename\" , \"Movie\" ); representation . put ( \"movieId\" , 1 ); variables . put ( \"representations\" , List . of ( representation )); DocumentContext context = queryExecutor . executeAndGetDocumentContext ( query , variables ); GraphQLResponse response = new GraphQLResponse ( context . jsonString ()); Movie movie = response . extractValueAsObject ( \"data._entities[0]\" , Movie . class ); assertThat ( movie . getScript (). getTitle ()). isEqualTo ( \"Top Secret\" ); }","title":"Testing Federated Queries without the Gateway"},{"location":"advanced/federated-testing/#using-the-entities-query-builder-api","text":"Alternatively, you can generate the federated query by using EntitiesGraphQLQuery to build the graphql request in combination with the code generation plugin to generate the classes needed to use the request builder. This provides a convenient type-safe way to build your queries. To set up code generation to generate the required classes to use for building your queries, follow the instructions here . You will also need to add com.netflix.graphql.dgs:graphql-dgs-client:latest.release dependency to build.gradle. Now we can write a test that uses EntitiesGraphQLQuery along with GraphQLQueryRequest and EntitiesProjectionRoot to build the query. Finally, you can also extract the response using GraphQLResponse . This set up is shown here: @Test void federatedMovieQueryAPI () throws IOException { // constructs the _entities query with variable $representations containing a // movie representation that represents { __typename: \"Movie\" movieId: 12345 } EntitiesGraphQLQuery entitiesQuery = new EntitiesGraphQLQuery . Builder () . addRepresentationAsVariable ( MovieRepresentation . newBuilder (). movieId ( 1122 ). build () ) . build (); // sets up the query and the field selection set using the EntitiesProjectionRoot GraphQLQueryRequest request = new GraphQLQueryRequest ( entitiesQuery , new EntitiesProjectionRoot (). onMovie (). movieId (). script (). title ()); String query = request . serialize (); // pass in the constructed _entities query with the variable map containing representations DocumentContext context = queryExecutor . executeAndGetDocumentContext ( query , entitiesQuery . getVariables ()); GraphQLResponse response = new GraphQLResponse ( context . jsonString ()); Movie movie = response . extractValueAsObject ( \"data._entities[0]\" , Movie . class ); assertThat ( movie . getScript (). getTitle ()). isEqualTo ( \"Top Secret\" ); } Check out this video for a demo on how to configure and write the above test.","title":"Using the Entities Query Builder API"},{"location":"advanced/file-uploads/","text":"In GraphQL, you model a file upload operation as a GraphQL mutation request from a client to your DGS. The following sections describe how you implement file uploads and downloads using a Multipart POST request. For more context on file uploads and best practices, see Apollo Server File Upload Best Practices by Khalil Stemmler from Apollo Blog . Multipart File Upload A multipart request is an HTTP request that contains multiple parts in a single request: the mutation query, file data, JSON objects, and whatever else you like. You can use Apollo\u2019s upload client, or even a simple cURL, to send along a stream of file data using a multipart request that you model in your schema as a Mutation. See GraphQL multipart request specification for the specification of a multipart POST request for uploading files using GraphQL mutations. The DGS framework supports the Upload scalar with which you can specify files in your mutation query as a MultipartFile . When you send a multipart request for file upload, the framework processes each part and assembles the final GraphQL query that it hands to your data fetcher for further processing. Here is an example of a Mutation query that uploads a file to your DGS: scalar Upload extend type Mutation { uploadScriptWithMultipartPOST(input: Upload!): Boolean } Note that you need to declare the Upload scalar in your schema, although the implementation is provided by the DGS framework. In your DGS, add a data fetcher to handle this as a MultipartFile as shown here: @DgsData ( parentType = DgsConstants . MUTATION . TYPE_NAME , field = \"uploadScriptWithMultipartPOST\" ) public boolean uploadScript ( DataFetchingEnvironment dfe ) throws IOException { // NOTE: Cannot use @InputArgument or Object Mapper to convert to class, because MultipartFile cannot be // deserialized MultipartFile file = dfe . getArgument ( \"input\" ); String content = new String ( file . getBytes ()); return ! content . isEmpty (); } Note that you will not be able to use a Jackson object mapper to deserialize a type that contains a MultipartFile , so you will need to explicitly get the file argument from your input. On your client, you can use apollo-upload-client to send your Mutation query as a multipart POST request with file data. Here\u2019s how you configure your link: import { createUploadLink } from 'apollo-upload-client' const uploadLink = createUploadLink ({ uri : uri }) const authedClient = authLink && new ApolloClient ({ link : uploadLink )), cache : new InMemoryCache () }) Once you set this up, set up your Mutation query and the pass the file that the user selected as a variable: // query for file uploads using multipart post const UploadScriptMultipartMutation_gql = gql ` mutation uploadScriptWithMultipartPOST($input: Upload!) { uploadScriptWithMultipartPOST(input: $input) } ` ; function MultipartScriptUpload () { const [ uploadScriptMultipartMutation , { loading : mutationLoading , error : mutationError , data : mutationData , }, ] = useMutation ( UploadScriptMultipartMutation_gql ); const [ scriptMultipartInput , setScriptMultipartInput ] = useState < any > (); const onSubmitScriptMultipart = () => { const fileInput = scriptMultipartInput . files [ 0 ]; uploadScriptMultipartMutation ({ variables : { input : fileInput }, }); }; return ( < div > < h3 > Upload script using multipart HTTP POST < /h3> < form onSubmit = { e => { e . preventDefault (); onSubmitScriptMultipart (); }} > < label > < input type = \"file\" ref = { ref => { setScriptMultipartInput ( ref ! ); }} /> < /label> < br /> < br /> < button type = \"submit\" > Submit < /button> < /form> < /div> ); }","title":"File Uploads"},{"location":"advanced/file-uploads/#multipart-file-upload","text":"A multipart request is an HTTP request that contains multiple parts in a single request: the mutation query, file data, JSON objects, and whatever else you like. You can use Apollo\u2019s upload client, or even a simple cURL, to send along a stream of file data using a multipart request that you model in your schema as a Mutation. See GraphQL multipart request specification for the specification of a multipart POST request for uploading files using GraphQL mutations. The DGS framework supports the Upload scalar with which you can specify files in your mutation query as a MultipartFile . When you send a multipart request for file upload, the framework processes each part and assembles the final GraphQL query that it hands to your data fetcher for further processing. Here is an example of a Mutation query that uploads a file to your DGS: scalar Upload extend type Mutation { uploadScriptWithMultipartPOST(input: Upload!): Boolean } Note that you need to declare the Upload scalar in your schema, although the implementation is provided by the DGS framework. In your DGS, add a data fetcher to handle this as a MultipartFile as shown here: @DgsData ( parentType = DgsConstants . MUTATION . TYPE_NAME , field = \"uploadScriptWithMultipartPOST\" ) public boolean uploadScript ( DataFetchingEnvironment dfe ) throws IOException { // NOTE: Cannot use @InputArgument or Object Mapper to convert to class, because MultipartFile cannot be // deserialized MultipartFile file = dfe . getArgument ( \"input\" ); String content = new String ( file . getBytes ()); return ! content . isEmpty (); } Note that you will not be able to use a Jackson object mapper to deserialize a type that contains a MultipartFile , so you will need to explicitly get the file argument from your input. On your client, you can use apollo-upload-client to send your Mutation query as a multipart POST request with file data. Here\u2019s how you configure your link: import { createUploadLink } from 'apollo-upload-client' const uploadLink = createUploadLink ({ uri : uri }) const authedClient = authLink && new ApolloClient ({ link : uploadLink )), cache : new InMemoryCache () }) Once you set this up, set up your Mutation query and the pass the file that the user selected as a variable: // query for file uploads using multipart post const UploadScriptMultipartMutation_gql = gql ` mutation uploadScriptWithMultipartPOST($input: Upload!) { uploadScriptWithMultipartPOST(input: $input) } ` ; function MultipartScriptUpload () { const [ uploadScriptMultipartMutation , { loading : mutationLoading , error : mutationError , data : mutationData , }, ] = useMutation ( UploadScriptMultipartMutation_gql ); const [ scriptMultipartInput , setScriptMultipartInput ] = useState < any > (); const onSubmitScriptMultipart = () => { const fileInput = scriptMultipartInput . files [ 0 ]; uploadScriptMultipartMutation ({ variables : { input : fileInput }, }); }; return ( < div > < h3 > Upload script using multipart HTTP POST < /h3> < form onSubmit = { e => { e . preventDefault (); onSubmitScriptMultipart (); }} > < label > < input type = \"file\" ref = { ref => { setScriptMultipartInput ( ref ! ); }} /> < /label> < br /> < br /> < button type = \"submit\" > Submit < /button> < /form> < /div> ); }","title":"Multipart File Upload"},{"location":"advanced/instrumentation/","text":"Adding instrumentation for tracing and logging It can be extremely valuable to add tracing, metrics and logging to your GraphQL API. At Netflix we publish tracing spans and metrics for each datafetcher to our distributed tracing/metrics backends, and log queries and query results to our logging backend. The implementations we use at Netflix are highly specific for our infrastructure, but it's easy to add your own to the framework! Internally the DGS framework uses GraphQL Java . GraphQL Java supports the concept of instrumentation . In the DGS framework we can easily add one or more instrumentation classes by implementing the graphql.execution.instrumentation.Instrumentation interface and register the class as @Component . The easiest way to implement the Instrumentation interface is to extend graphql.execution.instrumentation.SimpleInstrumentation . The following is an example of an implementation that outputs the execution time for each data fetcher, and the total query execution time, to the logs. Most likely you would want to replace the log output with writing to your tracing/metrics backend. Note that the code example accounts for async data fetchers. If we wouldn't do this, the result for an async data fetcher would always be 0, because the actual processing happens later. Java @Component public class ExampleTracingInstrumentation extends SimpleInstrumentation { private final static Logger LOGGER = LoggerFactory . getLogger ( ExampleTracingInstrumentation . class ); @Override public InstrumentationState createState () { return new TracingState (); } @Override public InstrumentationContext < ExecutionResult > beginExecution ( InstrumentationExecutionParameters parameters ) { TracingState tracingState = parameters . getInstrumentationState (); tracingState . startTime = System . currentTimeMillis (); return super . beginExecution ( parameters ); } @Override public DataFetcher <?> instrumentDataFetcher ( DataFetcher <?> dataFetcher , InstrumentationFieldFetchParameters parameters ) { // We only care about user code if ( parameters . isTrivialDataFetcher ()) { return dataFetcher ; } return environment -> { long startTime = System . currentTimeMillis (); Object result = dataFetcher . get ( environment ); if ( result instanceof CompletableFuture ) { (( CompletableFuture <?> ) result ). whenComplete (( r , ex ) -> { long totalTime = System . currentTimeMillis () - startTime ; LOGGER . info ( \"Async datafetcher {} took {}ms\" , findDatafetcherTag ( parameters ), totalTime ); }); } else { long totalTime = System . currentTimeMillis () - startTime ; LOGGER . info ( \"Datafetcher {} took {}ms\" , findDatafetcherTag ( parameters ), totalTime ); } return result ; }; } @Override public CompletableFuture < ExecutionResult > instrumentExecutionResult ( ExecutionResult executionResult , InstrumentationExecutionParameters parameters ) { TracingState tracingState = parameters . getInstrumentationState (); long totalTime = System . currentTimeMillis () - tracingState . startTime ; LOGGER . info ( \"Total execution time: {}ms\" , totalTime ); return super . instrumentExecutionResult ( executionResult , parameters ); } private String findDatafetcherTag ( InstrumentationFieldFetchParameters parameters ) { GraphQLOutputType type = parameters . getExecutionStepInfo (). getParent (). getType (); GraphQLObjectType parent ; if ( type instanceof GraphQLNonNull ) { parent = ( GraphQLObjectType ) (( GraphQLNonNull ) type ). getWrappedType (); } else { parent = ( GraphQLObjectType ) type ; } return parent . getName () + \".\" + parameters . getExecutionStepInfo (). getPath (). getSegmentName (); } static class TracingState implements InstrumentationState { long startTime ; } } Kotlin @Component class ExampleTracingInstrumentation : SimpleInstrumentation () { val logger : Logger = LoggerFactory . getLogger ( ExampleTracingInstrumentation :: class . java ) override fun createState (): InstrumentationState { return TraceState () } override fun beginExecution ( parameters : InstrumentationExecutionParameters ): InstrumentationContext < ExecutionResult > { val state : TraceState = parameters . getInstrumentationState () state . traceStartTime = System . currentTimeMillis () return super . beginExecution ( parameters ) } override fun instrumentDataFetcher ( dataFetcher : DataFetcher <*>, parameters : InstrumentationFieldFetchParameters ): DataFetcher <*> { // We only care about user code if ( parameters . isTrivialDataFetcher ) { return dataFetcher } val dataFetcherName = findDatafetcherTag ( parameters ) return DataFetcher { environment -> val startTime = System . currentTimeMillis () val result = dataFetcher . get ( environment ) if ( result is CompletableFuture <*>) { result . whenComplete { _ , _ -> val totalTime = System . currentTimeMillis () - startTime logger . info ( \"Async datafetcher '$dataFetcherName' took ${totalTime}ms\" ) } } else { val totalTime = System . currentTimeMillis () - startTime logger . info ( \"Datafetcher '$dataFetcherName': ${totalTime}ms\" ) } result } } override fun instrumentExecutionResult ( executionResult : ExecutionResult , parameters : InstrumentationExecutionParameters ): CompletableFuture < ExecutionResult > { val state : TraceState = parameters . getInstrumentationState () val totalTime = System . currentTimeMillis () - state . traceStartTime logger . info ( \"Total execution time: ${totalTime}ms\" ) return super . instrumentExecutionResult ( executionResult , parameters ) } private fun findDatafetcherTag ( parameters : InstrumentationFieldFetchParameters ): String { val type = parameters . executionStepInfo . parent . type val parentType = if ( type is GraphQLNonNull ) { type . wrappedType as GraphQLObjectType } else { type as GraphQLObjectType } return \"${parentType.name}.${parameters.executionStepInfo.path.segmentName}\" } data class TraceState ( var traceStartTime : Long = 0 ): InstrumentationState } Datafetcher 'Query.shows': 0ms Total execution time: 3ms","title":"Instrumentation (Tracing, Metrics)"},{"location":"advanced/instrumentation/#adding-instrumentation-for-tracing-and-logging","text":"It can be extremely valuable to add tracing, metrics and logging to your GraphQL API. At Netflix we publish tracing spans and metrics for each datafetcher to our distributed tracing/metrics backends, and log queries and query results to our logging backend. The implementations we use at Netflix are highly specific for our infrastructure, but it's easy to add your own to the framework! Internally the DGS framework uses GraphQL Java . GraphQL Java supports the concept of instrumentation . In the DGS framework we can easily add one or more instrumentation classes by implementing the graphql.execution.instrumentation.Instrumentation interface and register the class as @Component . The easiest way to implement the Instrumentation interface is to extend graphql.execution.instrumentation.SimpleInstrumentation . The following is an example of an implementation that outputs the execution time for each data fetcher, and the total query execution time, to the logs. Most likely you would want to replace the log output with writing to your tracing/metrics backend. Note that the code example accounts for async data fetchers. If we wouldn't do this, the result for an async data fetcher would always be 0, because the actual processing happens later. Java @Component public class ExampleTracingInstrumentation extends SimpleInstrumentation { private final static Logger LOGGER = LoggerFactory . getLogger ( ExampleTracingInstrumentation . class ); @Override public InstrumentationState createState () { return new TracingState (); } @Override public InstrumentationContext < ExecutionResult > beginExecution ( InstrumentationExecutionParameters parameters ) { TracingState tracingState = parameters . getInstrumentationState (); tracingState . startTime = System . currentTimeMillis (); return super . beginExecution ( parameters ); } @Override public DataFetcher <?> instrumentDataFetcher ( DataFetcher <?> dataFetcher , InstrumentationFieldFetchParameters parameters ) { // We only care about user code if ( parameters . isTrivialDataFetcher ()) { return dataFetcher ; } return environment -> { long startTime = System . currentTimeMillis (); Object result = dataFetcher . get ( environment ); if ( result instanceof CompletableFuture ) { (( CompletableFuture <?> ) result ). whenComplete (( r , ex ) -> { long totalTime = System . currentTimeMillis () - startTime ; LOGGER . info ( \"Async datafetcher {} took {}ms\" , findDatafetcherTag ( parameters ), totalTime ); }); } else { long totalTime = System . currentTimeMillis () - startTime ; LOGGER . info ( \"Datafetcher {} took {}ms\" , findDatafetcherTag ( parameters ), totalTime ); } return result ; }; } @Override public CompletableFuture < ExecutionResult > instrumentExecutionResult ( ExecutionResult executionResult , InstrumentationExecutionParameters parameters ) { TracingState tracingState = parameters . getInstrumentationState (); long totalTime = System . currentTimeMillis () - tracingState . startTime ; LOGGER . info ( \"Total execution time: {}ms\" , totalTime ); return super . instrumentExecutionResult ( executionResult , parameters ); } private String findDatafetcherTag ( InstrumentationFieldFetchParameters parameters ) { GraphQLOutputType type = parameters . getExecutionStepInfo (). getParent (). getType (); GraphQLObjectType parent ; if ( type instanceof GraphQLNonNull ) { parent = ( GraphQLObjectType ) (( GraphQLNonNull ) type ). getWrappedType (); } else { parent = ( GraphQLObjectType ) type ; } return parent . getName () + \".\" + parameters . getExecutionStepInfo (). getPath (). getSegmentName (); } static class TracingState implements InstrumentationState { long startTime ; } } Kotlin @Component class ExampleTracingInstrumentation : SimpleInstrumentation () { val logger : Logger = LoggerFactory . getLogger ( ExampleTracingInstrumentation :: class . java ) override fun createState (): InstrumentationState { return TraceState () } override fun beginExecution ( parameters : InstrumentationExecutionParameters ): InstrumentationContext < ExecutionResult > { val state : TraceState = parameters . getInstrumentationState () state . traceStartTime = System . currentTimeMillis () return super . beginExecution ( parameters ) } override fun instrumentDataFetcher ( dataFetcher : DataFetcher <*>, parameters : InstrumentationFieldFetchParameters ): DataFetcher <*> { // We only care about user code if ( parameters . isTrivialDataFetcher ) { return dataFetcher } val dataFetcherName = findDatafetcherTag ( parameters ) return DataFetcher { environment -> val startTime = System . currentTimeMillis () val result = dataFetcher . get ( environment ) if ( result is CompletableFuture <*>) { result . whenComplete { _ , _ -> val totalTime = System . currentTimeMillis () - startTime logger . info ( \"Async datafetcher '$dataFetcherName' took ${totalTime}ms\" ) } } else { val totalTime = System . currentTimeMillis () - startTime logger . info ( \"Datafetcher '$dataFetcherName': ${totalTime}ms\" ) } result } } override fun instrumentExecutionResult ( executionResult : ExecutionResult , parameters : InstrumentationExecutionParameters ): CompletableFuture < ExecutionResult > { val state : TraceState = parameters . getInstrumentationState () val totalTime = System . currentTimeMillis () - state . traceStartTime logger . info ( \"Total execution time: ${totalTime}ms\" ) return super . instrumentExecutionResult ( executionResult , parameters ) } private fun findDatafetcherTag ( parameters : InstrumentationFieldFetchParameters ): String { val type = parameters . executionStepInfo . parent . type val parentType = if ( type is GraphQLNonNull ) { type . wrappedType as GraphQLObjectType } else { type as GraphQLObjectType } return \"${parentType.name}.${parameters.executionStepInfo.path.segmentName}\" } data class TraceState ( var traceStartTime : Long = 0 ): InstrumentationState } Datafetcher 'Query.shows': 0ms Total execution time: 3ms","title":"Adding instrumentation for tracing and logging"},{"location":"advanced/java-client/","text":"Usage The DGS framework provides a GraphQL client that can be used to retrieve data from a GraphQL endpoint. The client has two components, each usable by itself, or in combination together. GraphQLClient - A HTTP client wrapper that provides easy parsing of GraphQL responses Query API codegen - Generate type-safe Query builders HTTP client wrapper The GraphQL client wraps any HTTP client and provides easy parsing of GraphQL responses. The client can be used against any GraphQL endpoint (it doesn't have to be implemented with the DGS framework), but provides extra conveniences for parsing Gateway and DGS responses. This includes support for the Errors Spec . To use the client, create an instance of DefaultGraphQLClient . GraphQLClient client = new DefaultGraphQLClient ( url ); The url is the server url of the endpoint you want to call. This url will be passed down to the callback discussed below. Using the GraphQLClient a query can be executed. The executeQuery method has three arguments: The query String An optional map of query variables An instance of RequestExecutor , typically provided as a lambda. Because of the large number HTTP clients in use within Netflix, the GraphQLClient is decoupled from any particular HTTP client implementation. Any HTTP client (RestTemplate, RestClient, OkHTTP, ....) can be used. The developer is responsible for making the actual HTTP call by implementing a RequestExecutor . RequestExecutor receives the url , a map of headers and the request body as parameters, and should return an instance of HttpResponse . Based on the HTTP response the GraphQLClient parses the response and provides easy access to data and errors. The example below uses RestTemplate . private RestTemplate dgsRestTemplate; private static final String URL = \"http://someserver/graphql\"; private static final String QUERY = \"{\\n\" + \" ticks(first: %d, after:%d){\\n\" + \" edges {\\n\" + \" node {\\n\" + \" route {\\n\" + \" name\\n\" + \" grade\\n\" + \" pitches\\n\" + \" location\\n\" + \" }\\n\" + \" \\n\" + \" userStars\\n\" + \" }\\n\" + \" }\\n\" + \" }\\n\" + \"}\"; public List<TicksConnection> getData() { DefaultGraphQLClient graphQLClient = new DefaultGraphQLClient(URL); GraphQLResponse response = graphQLClient.executeQuery(query, new HashMap<>(), (url, headers, body) -> { /** * The requestHeaders providers headers typically required to call a GraphQL endpoint, including the Accept and Content-Type headers. * To use RestTemplate, the requestHeaders need to be transformed into Spring's HttpHeaders. */ HttpHeaders requestHeaders = new HttpHeaders(); headers.forEach(requestHeaders::put); /** * Use RestTemplate to call the GraphQL service. * The response type should simply be String, because the parsing will be done by the GraphQLClient. */ ResponseEntity<String> exchange = dgsRestTemplate.exchange(url, HttpMethod.POST, new HttpEntity(body, requestHeaders), String.class); /** * Return a HttpResponse, which contains the HTTP status code and response body (as a String). * The way to get these depend on the HTTP client. */ return new HttpResponse(exchange.getStatusCodeValue(), exchange.getBody()); }); TicksConnection ticks = graphQLResponse.extractValueAsObject(\"ticks\", TicksConnection.class); return ticks; } The GraphQLClient provides methods to parse and retrieve data and errors in a variety of ways. Refer to the GrqphQLClient JavaDoc for the complete list of supported methods. method description example getData Get the data as a Map Map<String,Object> data = response.getData() dataAsObject Parse data as the provided class, using the Jackson Object Mapper TickResponse data = response.dataAsObject(TicksResponse.class) extractValue Extract values given a JsonPath . The return type will be whatever type you expect, but depends on the JSON shape. For JSON objects, a Map is returned. Although this looks type safe, it really isn't. It's mostly useful for \"simple\" types like String, Int etc., and Lists of those types. List<String> name = response.extractValue(\"movies[*].originalTitle\") extractValueAsObject Extract values given a JsonPath and deserialize into the given class Ticks ticks = response.extractValueAsObject(\"ticks\", Ticks.class) extractValueAsObject Extract values given a JsonPath and deserialize into the given TypeRef. Useful for Maps and Lists of a certain class. List<Route> routes = response.extractValueAsObject(\"ticks.edges[*].node.route\", new TypeRef<List<Route>>(){}) getRequestDetails Extract a RequestDetails object. This only works if requestDetails was requested in the query, and against the Gateway. RequestDetails requestDetails = response.getRequestDetails() getParsed Get the parsed DocumentContext for further JsonPath processing response.getDocumentContext() Errors The GraphQLClient checks both for HTTP level errors (based on the response status code) and the errors block in a GraphQL response. The GraphQLClient is compatible with the Errors Spec used by the Gateway and DGS, and makes it easy to extract error information such as the ErrorType. For example, for following GraphQL response the GraphQLClient lets you easily get the ErrorType and ErrorDetail fields. Note that the ErrorType is an enum as specified by the Errors Spec . { \"errors\": [ { \"message\": \"java.lang.RuntimeException: test\", \"locations\": [], \"path\": [ \"hello\" ], \"extensions\": { \"errorType\": \"BAD_REQUEST\", \"errorDetail\": \"FIELD_NOT_FOUND\" } } ], \"data\": { \"hello\": null } } assertThat ( graphQLResponse . errors . get ( 0 ). extensions . errorType ). isEqualTo ( ErrorType . BAD_REQUEST ) assertThat ( graphQLResponse . errors . get ( 0 ). extensions . errorDetail ). isEqualTo ( \"FIELD_NOT_FOUND\" ) Type safe Query API Based on a GraphQL schema a type safe query API can be generated for Java/Kotlin. The generated API is a builder style API that lets you build a GraphQL query and it's projection (field selection). Because the code gets re-generated when the schema changes, it helps catch errors in the query. Because Java doesn't support multi-line strings (yet) it's also arguably a more readable way to specify a query. If you own a DGS and want to generate a client for this DGS (e.g. for testing purposes) the client generation is just an extra property on the Codegen configuration . Specify the following in your build.gradle . buildscript { dependencies { classpath 'netflix:graphql-dgs-codegen-gradle:latest.release' } } apply plugin: 'codegen-gradle-plugin' generateJava { packageName = 'com.example.packagename' // The package name to use to generate sources generateClient = true } Code will be generated on build. The generated code is in build/generated . With codegen configured correctly, a builder style API will be generated when building the project. Using the same query example as above, the query can be build using the generated builder API. GraphQLQueryRequest graphQLQueryRequest = new GraphQLQueryRequest( new TicksGraphQLQuery.Builder() .first(first) .after(after) .build(), new TicksConnectionProjectionRoot() .edges() .node() .date() .route() .name() .votes() .starRating() .parent() .grade()); String query = graphQLQueryRequest.serialize(); The GraphQLQueryRequest is a class from graphql-dgs-client . The TicksGraphQLQuery and TicksConnectionProjectionRoot are generated. After building the query, it can be serialized to a String, and executed using the GraphQLClient. Note that the edges and node fields are because the example schema is using Relay pagination. Interface projections When a field returns an interface, fields on the concrete types are specified using a fragment. type Query @extends { script(name: String): Script } interface Script { title: String director: String actors: [Actor] } type MovieScript implements Script { title: String director: String length: Int } type ShowScript implements Script { title: String director: String episodes: Int } query { script(name: \"Top Secret\") { title ... on MovieScript { length } } } This syntax is supported by the Query builder as well. GraphQLQueryRequest graphQLQueryRequest = new GraphQLQueryRequest ( new ScriptGraphQLQuery . Builder () . name ( \"Top Secret\" ) . build (), new ScriptProjectionRoot () . title () . onMovieScript () . length (); ); Building Federated Queries You can use GraphQLQueryRequest along with EntitiesGraphQLQuery to generated federated queries. The API provides a type-safe way to construct the _entities query with the associated representations based on the input schema. The representations are passed in as a map of variables. Each representation class is generated based on the key fields defined on the entity in your schema, along with the __typename . The EntitiesProjectionRoot is used to select query fields on the specified type. For example, let us look at a schema that extends a Movie type: type Movie @key(fields: \"movieId\") @extends { movieId: Int @external script: MovieScript } type MovieScript { title: String director: String actors: [Actor] } type Actor { name: String gender: String age: Int } With client code generation, you will now have a MovieRepresentation containing the key field, i.e., movieId , and the __typename field already set to type Movie . Now you can add each representation to the EntitiesGraphQLQuery as a representations variable. You will also have a EntitiesProjectionRoot with onMovie() to select fields on Movie from. Finally, you put them all together as a GraphQLQueryRequest , which you serialize into the final query string. The map of representations variables is available via getVariables on the EntitiesGraphQLQuery . Here is an example for the schema shown earlier: EntitiesGraphQLQuery entitiesQuery = new EntitiesGraphQLQuery . Builder () . addRepresentationAsVariable ( MovieRepresentation . newBuilder (). movieId ( 1122 ). build () ) . build (); GraphQLQueryRequest request = new GraphQLQueryRequest ( entitiesQuery , new EntitiesProjectionRoot (). onMovie (). movieId (). script (). title () ); String query = request . serialize (); Map < String , Object > representations = entitiesQuery . getVariables ();","title":"Java GraphQL Client"},{"location":"advanced/java-client/#usage","text":"The DGS framework provides a GraphQL client that can be used to retrieve data from a GraphQL endpoint. The client has two components, each usable by itself, or in combination together. GraphQLClient - A HTTP client wrapper that provides easy parsing of GraphQL responses Query API codegen - Generate type-safe Query builders","title":"Usage"},{"location":"advanced/java-client/#http-client-wrapper","text":"The GraphQL client wraps any HTTP client and provides easy parsing of GraphQL responses. The client can be used against any GraphQL endpoint (it doesn't have to be implemented with the DGS framework), but provides extra conveniences for parsing Gateway and DGS responses. This includes support for the Errors Spec . To use the client, create an instance of DefaultGraphQLClient . GraphQLClient client = new DefaultGraphQLClient ( url ); The url is the server url of the endpoint you want to call. This url will be passed down to the callback discussed below. Using the GraphQLClient a query can be executed. The executeQuery method has three arguments: The query String An optional map of query variables An instance of RequestExecutor , typically provided as a lambda. Because of the large number HTTP clients in use within Netflix, the GraphQLClient is decoupled from any particular HTTP client implementation. Any HTTP client (RestTemplate, RestClient, OkHTTP, ....) can be used. The developer is responsible for making the actual HTTP call by implementing a RequestExecutor . RequestExecutor receives the url , a map of headers and the request body as parameters, and should return an instance of HttpResponse . Based on the HTTP response the GraphQLClient parses the response and provides easy access to data and errors. The example below uses RestTemplate . private RestTemplate dgsRestTemplate; private static final String URL = \"http://someserver/graphql\"; private static final String QUERY = \"{\\n\" + \" ticks(first: %d, after:%d){\\n\" + \" edges {\\n\" + \" node {\\n\" + \" route {\\n\" + \" name\\n\" + \" grade\\n\" + \" pitches\\n\" + \" location\\n\" + \" }\\n\" + \" \\n\" + \" userStars\\n\" + \" }\\n\" + \" }\\n\" + \" }\\n\" + \"}\"; public List<TicksConnection> getData() { DefaultGraphQLClient graphQLClient = new DefaultGraphQLClient(URL); GraphQLResponse response = graphQLClient.executeQuery(query, new HashMap<>(), (url, headers, body) -> { /** * The requestHeaders providers headers typically required to call a GraphQL endpoint, including the Accept and Content-Type headers. * To use RestTemplate, the requestHeaders need to be transformed into Spring's HttpHeaders. */ HttpHeaders requestHeaders = new HttpHeaders(); headers.forEach(requestHeaders::put); /** * Use RestTemplate to call the GraphQL service. * The response type should simply be String, because the parsing will be done by the GraphQLClient. */ ResponseEntity<String> exchange = dgsRestTemplate.exchange(url, HttpMethod.POST, new HttpEntity(body, requestHeaders), String.class); /** * Return a HttpResponse, which contains the HTTP status code and response body (as a String). * The way to get these depend on the HTTP client. */ return new HttpResponse(exchange.getStatusCodeValue(), exchange.getBody()); }); TicksConnection ticks = graphQLResponse.extractValueAsObject(\"ticks\", TicksConnection.class); return ticks; } The GraphQLClient provides methods to parse and retrieve data and errors in a variety of ways. Refer to the GrqphQLClient JavaDoc for the complete list of supported methods. method description example getData Get the data as a Map Map<String,Object> data = response.getData() dataAsObject Parse data as the provided class, using the Jackson Object Mapper TickResponse data = response.dataAsObject(TicksResponse.class) extractValue Extract values given a JsonPath . The return type will be whatever type you expect, but depends on the JSON shape. For JSON objects, a Map is returned. Although this looks type safe, it really isn't. It's mostly useful for \"simple\" types like String, Int etc., and Lists of those types. List<String> name = response.extractValue(\"movies[*].originalTitle\") extractValueAsObject Extract values given a JsonPath and deserialize into the given class Ticks ticks = response.extractValueAsObject(\"ticks\", Ticks.class) extractValueAsObject Extract values given a JsonPath and deserialize into the given TypeRef. Useful for Maps and Lists of a certain class. List<Route> routes = response.extractValueAsObject(\"ticks.edges[*].node.route\", new TypeRef<List<Route>>(){}) getRequestDetails Extract a RequestDetails object. This only works if requestDetails was requested in the query, and against the Gateway. RequestDetails requestDetails = response.getRequestDetails() getParsed Get the parsed DocumentContext for further JsonPath processing response.getDocumentContext()","title":"HTTP client wrapper"},{"location":"advanced/java-client/#errors","text":"The GraphQLClient checks both for HTTP level errors (based on the response status code) and the errors block in a GraphQL response. The GraphQLClient is compatible with the Errors Spec used by the Gateway and DGS, and makes it easy to extract error information such as the ErrorType. For example, for following GraphQL response the GraphQLClient lets you easily get the ErrorType and ErrorDetail fields. Note that the ErrorType is an enum as specified by the Errors Spec . { \"errors\": [ { \"message\": \"java.lang.RuntimeException: test\", \"locations\": [], \"path\": [ \"hello\" ], \"extensions\": { \"errorType\": \"BAD_REQUEST\", \"errorDetail\": \"FIELD_NOT_FOUND\" } } ], \"data\": { \"hello\": null } } assertThat ( graphQLResponse . errors . get ( 0 ). extensions . errorType ). isEqualTo ( ErrorType . BAD_REQUEST ) assertThat ( graphQLResponse . errors . get ( 0 ). extensions . errorDetail ). isEqualTo ( \"FIELD_NOT_FOUND\" )","title":"Errors"},{"location":"advanced/java-client/#type-safe-query-api","text":"Based on a GraphQL schema a type safe query API can be generated for Java/Kotlin. The generated API is a builder style API that lets you build a GraphQL query and it's projection (field selection). Because the code gets re-generated when the schema changes, it helps catch errors in the query. Because Java doesn't support multi-line strings (yet) it's also arguably a more readable way to specify a query. If you own a DGS and want to generate a client for this DGS (e.g. for testing purposes) the client generation is just an extra property on the Codegen configuration . Specify the following in your build.gradle . buildscript { dependencies { classpath 'netflix:graphql-dgs-codegen-gradle:latest.release' } } apply plugin: 'codegen-gradle-plugin' generateJava { packageName = 'com.example.packagename' // The package name to use to generate sources generateClient = true } Code will be generated on build. The generated code is in build/generated . With codegen configured correctly, a builder style API will be generated when building the project. Using the same query example as above, the query can be build using the generated builder API. GraphQLQueryRequest graphQLQueryRequest = new GraphQLQueryRequest( new TicksGraphQLQuery.Builder() .first(first) .after(after) .build(), new TicksConnectionProjectionRoot() .edges() .node() .date() .route() .name() .votes() .starRating() .parent() .grade()); String query = graphQLQueryRequest.serialize(); The GraphQLQueryRequest is a class from graphql-dgs-client . The TicksGraphQLQuery and TicksConnectionProjectionRoot are generated. After building the query, it can be serialized to a String, and executed using the GraphQLClient. Note that the edges and node fields are because the example schema is using Relay pagination.","title":"Type safe Query API"},{"location":"advanced/java-client/#interface-projections","text":"When a field returns an interface, fields on the concrete types are specified using a fragment. type Query @extends { script(name: String): Script } interface Script { title: String director: String actors: [Actor] } type MovieScript implements Script { title: String director: String length: Int } type ShowScript implements Script { title: String director: String episodes: Int } query { script(name: \"Top Secret\") { title ... on MovieScript { length } } } This syntax is supported by the Query builder as well. GraphQLQueryRequest graphQLQueryRequest = new GraphQLQueryRequest ( new ScriptGraphQLQuery . Builder () . name ( \"Top Secret\" ) . build (), new ScriptProjectionRoot () . title () . onMovieScript () . length (); );","title":"Interface projections"},{"location":"advanced/java-client/#building-federated-queries","text":"You can use GraphQLQueryRequest along with EntitiesGraphQLQuery to generated federated queries. The API provides a type-safe way to construct the _entities query with the associated representations based on the input schema. The representations are passed in as a map of variables. Each representation class is generated based on the key fields defined on the entity in your schema, along with the __typename . The EntitiesProjectionRoot is used to select query fields on the specified type. For example, let us look at a schema that extends a Movie type: type Movie @key(fields: \"movieId\") @extends { movieId: Int @external script: MovieScript } type MovieScript { title: String director: String actors: [Actor] } type Actor { name: String gender: String age: Int } With client code generation, you will now have a MovieRepresentation containing the key field, i.e., movieId , and the __typename field already set to type Movie . Now you can add each representation to the EntitiesGraphQLQuery as a representations variable. You will also have a EntitiesProjectionRoot with onMovie() to select fields on Movie from. Finally, you put them all together as a GraphQLQueryRequest , which you serialize into the final query string. The map of representations variables is available via getVariables on the EntitiesGraphQLQuery . Here is an example for the schema shown earlier: EntitiesGraphQLQuery entitiesQuery = new EntitiesGraphQLQuery . Builder () . addRepresentationAsVariable ( MovieRepresentation . newBuilder (). movieId ( 1122 ). build () ) . build (); GraphQLQueryRequest request = new GraphQLQueryRequest ( entitiesQuery , new EntitiesProjectionRoot (). onMovie (). movieId (). script (). title () ); String query = request . serialize (); Map < String , Object > representations = entitiesQuery . getVariables ();","title":"Building Federated Queries"},{"location":"advanced/mocking/","text":"This guide is about how to provide mock data for data fetchers. There are two primary reasons to do so: Provide example data that UI teams can use while the data fetcher is under development. This is useful during schema design. Provide stable test data for UI teams to write their tests against. An argument can be made that this type of mock data should live in the UI code. It\u2019s for their tests after all. However, by pulling it into the [DGS], the owners of the data can provide test data that can be used by many teams. The two approaches are also not mutually exclusive. [GraphQL] Mocking The library in the [DGS] framework supports: returning static data from mocks returning generated data for simple types (like String fields) The library is modular, so you can use it for a variety of workflows and use cases. The mocking framework is already part of the DGS framework. All you need to provide is one or more MockProvider implementations. MockProvider is an interface with a Map<String, Object> provide() method. Each key in the Map is a field in the [GraphQL] schema, which can be several levels deep. The value in the Map is whatever mock data you want to return for this key. Example Create a MockProvider that provides mock data for the hello field you created in the getting started tutorial : @Component public class HelloMockProvider implements MockProvider { @NotNull @Override public Map < String , Object > provide () { Map < String , Object > mock = new HashMap <> (); mock . put ( \"hello\" , \"Mocked hello response\" ); return mock ; } } If you run the application again and test the hello query, you will see that it now returns the mock data. This is useful while a data fetcher isn\u2019t implemented yet, but the true power comes from enabling the mocks remotely.","title":"Mocking"},{"location":"advanced/mocking/#graphql-mocking","text":"The library in the [DGS] framework supports: returning static data from mocks returning generated data for simple types (like String fields) The library is modular, so you can use it for a variety of workflows and use cases. The mocking framework is already part of the DGS framework. All you need to provide is one or more MockProvider implementations. MockProvider is an interface with a Map<String, Object> provide() method. Each key in the Map is a field in the [GraphQL] schema, which can be several levels deep. The value in the Map is whatever mock data you want to return for this key.","title":"[GraphQL] Mocking"},{"location":"advanced/mocking/#example","text":"Create a MockProvider that provides mock data for the hello field you created in the getting started tutorial : @Component public class HelloMockProvider implements MockProvider { @NotNull @Override public Map < String , Object > provide () { Map < String , Object > mock = new HashMap <> (); mock . put ( \"hello\" , \"Mocked hello response\" ); return mock ; } } If you run the application again and test the hello query, you will see that it now returns the mock data. This is useful while a data fetcher isn\u2019t implemented yet, but the true power comes from enabling the mocks remotely.","title":"Example"},{"location":"advanced/schema-from-code/","text":"Use a schema-first approach for GraphQL in most cases. Most DGSs have a schema file and use the declarative, annotation based, programming model to create data fetchers and such. Creating a Schema from Code during Startup There are scenarios however in which it is better to create a schema from code, during startup of the application. An example is a GraphQL schema which is derived from another schema, or in code generation scenarios. The DGS framework supports this by letting users register a TypeDefinitionRegistry and GraphQLCodeRegistry , both concepts from the graphql-java library. The TypeDefinitionRegistry represents a schema and GraphQLCodeRegistry the mapping from fields to data fetchers. Both are concepts from the graphql-java core library. The types defined in a provided TypeDefinitionRegistry are merged with any schema files when available . A combination of data fetchers registered using annotations and a GraphQLCodeRegistry works as well. The following is an example of creating a TypeDefinitionRegistry . @Configuration public class ExtraTypeDefinitionRegistry { @Bean public TypeDefinitionRegistry registry () { ObjectTypeExtensionDefinition objectTypeExtensionDefinition = ObjectTypeExtensionDefinition . newObjectTypeExtensionDefinition (). name ( \"Query\" ). fieldDefinition ( FieldDefinition . newFieldDefinition (). name ( \"myField\" ). type ( new TypeName ( \"String\" )). build ()) . build (); TypeDefinitionRegistry typeDefinitionRegistry = new TypeDefinitionRegistry (); typeDefinitionRegistry . add ( objectTypeExtensionDefinition ); return typeDefinitionRegistry ; } } This TypeDefinitionRegistry creates a field myField on the Query object type. This means there also needs to be a datafetcher for this field, which is created using the @DgsCodeRegistry annotation. @DgsComponent public class ExtraCodeRegistry { @Autowired DgsDataFetcherFactory dgsDataFetcherFactory ; @DgsCodeRegistry public GraphQLCodeRegistry . Builder registry ( GraphQLCodeRegistry . Builder codeRegistryBuilder , TypeDefinitionRegistry registry ) { DataFetcher < String > df = ( dfe ) -> \"yes, my extra field!\" ; FieldCoordinates coordinates = FieldCoordinates . coordinates ( \"Query\" , \"myField\" ); DataFetcher < Object > dgsDataFetcher = dgsDataFetcherFactory . createDataFetcher ( coordinates , df ); return codeRegistryBuilder . dataFetcher ( coordinates , dgsDataFetcher ); } } Note how this example calls a method of the DgsDataFetcherFactory to create the data fetcher. By using this factory, the data fetcher gets all the extra features that DGS data fetchers have, such as tracing.","title":"Generating Schema from Code"},{"location":"advanced/schema-from-code/#creating-a-schema-from-code-during-startup","text":"There are scenarios however in which it is better to create a schema from code, during startup of the application. An example is a GraphQL schema which is derived from another schema, or in code generation scenarios. The DGS framework supports this by letting users register a TypeDefinitionRegistry and GraphQLCodeRegistry , both concepts from the graphql-java library. The TypeDefinitionRegistry represents a schema and GraphQLCodeRegistry the mapping from fields to data fetchers. Both are concepts from the graphql-java core library. The types defined in a provided TypeDefinitionRegistry are merged with any schema files when available . A combination of data fetchers registered using annotations and a GraphQLCodeRegistry works as well. The following is an example of creating a TypeDefinitionRegistry . @Configuration public class ExtraTypeDefinitionRegistry { @Bean public TypeDefinitionRegistry registry () { ObjectTypeExtensionDefinition objectTypeExtensionDefinition = ObjectTypeExtensionDefinition . newObjectTypeExtensionDefinition (). name ( \"Query\" ). fieldDefinition ( FieldDefinition . newFieldDefinition (). name ( \"myField\" ). type ( new TypeName ( \"String\" )). build ()) . build (); TypeDefinitionRegistry typeDefinitionRegistry = new TypeDefinitionRegistry (); typeDefinitionRegistry . add ( objectTypeExtensionDefinition ); return typeDefinitionRegistry ; } } This TypeDefinitionRegistry creates a field myField on the Query object type. This means there also needs to be a datafetcher for this field, which is created using the @DgsCodeRegistry annotation. @DgsComponent public class ExtraCodeRegistry { @Autowired DgsDataFetcherFactory dgsDataFetcherFactory ; @DgsCodeRegistry public GraphQLCodeRegistry . Builder registry ( GraphQLCodeRegistry . Builder codeRegistryBuilder , TypeDefinitionRegistry registry ) { DataFetcher < String > df = ( dfe ) -> \"yes, my extra field!\" ; FieldCoordinates coordinates = FieldCoordinates . coordinates ( \"Query\" , \"myField\" ); DataFetcher < Object > dgsDataFetcher = dgsDataFetcherFactory . createDataFetcher ( coordinates , df ); return codeRegistryBuilder . dataFetcher ( coordinates , dgsDataFetcher ); } } Note how this example calls a method of the DgsDataFetcherFactory to create the data fetcher. By using this factory, the data fetcher gets all the extra features that DGS data fetchers have, such as tracing.","title":"Creating a Schema from Code during Startup"},{"location":"advanced/security/","text":"Fine-grained Access Control with @Secured The DGS Framework integrates with Spring Security using the well known @Secured annotation. Spring Security itself can be configured in many ways, which goes beyond the scope of this documentation. Once Spring Security is set up however, you can apply @Secured to your data fetchers, very similarly to how you apply it to a REST Controller in Spring MVC. @DgsComponent public class SecurityExampleFetchers { @DgsData ( parentType = \"Query\" , field = \"hello\" ) public String hello () { return \"Hello to everyone\" ; } @Secured ( \"admin\" ) @DgsData ( parentType = \"Query\" , field = \"secureGroup\" ) public String secureGroup () { return \"Hello to admins only\" ; } }","title":"Security"},{"location":"advanced/security/#fine-grained-access-control-with-secured","text":"The DGS Framework integrates with Spring Security using the well known @Secured annotation. Spring Security itself can be configured in many ways, which goes beyond the scope of this documentation. Once Spring Security is set up however, you can apply @Secured to your data fetchers, very similarly to how you apply it to a REST Controller in Spring MVC. @DgsComponent public class SecurityExampleFetchers { @DgsData ( parentType = \"Query\" , field = \"hello\" ) public String hello () { return \"Hello to everyone\" ; } @Secured ( \"admin\" ) @DgsData ( parentType = \"Query\" , field = \"secureGroup\" ) public String secureGroup () { return \"Hello to admins only\" ; } }","title":"Fine-grained Access Control with @Secured"},{"location":"advanced/subscriptions/","text":"GraphQL Subscriptions are used to receive updates for a query from the server over time. A common example is sending update notifications from the server. Regular GraphQL queries use a simple (HTTP) request/response to execute a query. For subscriptions a connection is kept open. Currently, we support subscriptions using Websockets. We will add support for SSE in the future. The Server Side Programming Model In the DGS framework a Subscription is implemented as a data fetcher with the @DgsData annotation. The difference with a normal data fetcher is that a subscription must return a org.reactivestreams.Publisher . import reactor.core.publisher.Flux ; import org.reactivestreams.Publisher ; \u22ee @DgsData ( parentType = \"Subscription\" , field = \"stocks\" ) public Publisher < Stock > stocks () { return Flux . interval ( Duration . ofSeconds ( 1 )). map ({ t -> Tick ( t . toString ()) }) } The Publisher interface is from Reactive Streams. Flux is the default implementation for Spring. A complete example can be found in SubscriptionDatafetcher.java . Next, a transport implementation must be chosen , which depends on how your app is deployed . WebSockets The most common transport protocol for Subscriptions in the GraphQL community is WebSockets. Apollo defines a sub-protocol , which is supported by client libraries and implemented by the DGS framework. To enable WebSockets support, add the following module to your build.gradle : implementation 'com.netflix.graphql.dgs:graphql-dgs-subscriptions-websockets-autoconfigure:latest.release' Apollo client supports WebSockets through a link . Typically you want to configure Apollo Client with both an HTTP link and a WS link, and split between them based on the query type.","title":"Subscriptions"},{"location":"advanced/subscriptions/#the-server-side-programming-model","text":"In the DGS framework a Subscription is implemented as a data fetcher with the @DgsData annotation. The difference with a normal data fetcher is that a subscription must return a org.reactivestreams.Publisher . import reactor.core.publisher.Flux ; import org.reactivestreams.Publisher ; \u22ee @DgsData ( parentType = \"Subscription\" , field = \"stocks\" ) public Publisher < Stock > stocks () { return Flux . interval ( Duration . ofSeconds ( 1 )). map ({ t -> Tick ( t . toString ()) }) } The Publisher interface is from Reactive Streams. Flux is the default implementation for Spring. A complete example can be found in SubscriptionDatafetcher.java . Next, a transport implementation must be chosen , which depends on how your app is deployed .","title":"The Server Side Programming Model"},{"location":"advanced/subscriptions/#websockets","text":"The most common transport protocol for Subscriptions in the GraphQL community is WebSockets. Apollo defines a sub-protocol , which is supported by client libraries and implemented by the DGS framework. To enable WebSockets support, add the following module to your build.gradle : implementation 'com.netflix.graphql.dgs:graphql-dgs-subscriptions-websockets-autoconfigure:latest.release' Apollo client supports WebSockets through a link . Typically you want to configure Apollo Client with both an HTTP link and a WS link, and split between them based on the query type.","title":"WebSockets"},{"location":"advanced/type-resolvers-for-abstract-types/","text":"You must register type resolvers whenever you use interface types or union types in your schema. Interface types and union types are explained in the GraphQL documentation . As an example, the following schema defines a Movie interface type with two different concrete object type implementations. type Query { movies: [Movie] } interface Movie { title: String } type ScaryMovie implements Movie { title: String gory: Boolean scareFactor: Int } type ActionMovie implements Movie { title: String nrOfExplosions: Int } The following data fetcher is registered to return a list of movies. The data fetcher returns a combination Movie types. @DgsComponent public class MovieDataFetcher { @DgsData ( parentType = \"Query\" , field = \"movies\" ) public List < Movie > movies () { return Lists . newArrayList ( new ActionMovie ( \"Crouching Tiger\" , 0 ), new ActionMovie ( \"Black hawk down\" , 10 ), new ScaryMovie ( \"American Horror Story\" , true , 10 ), new ScaryMovie ( \"Love Death + Robots\" , false , 4 ) ); } } The GraphQL runtime needs to know that a Java instance of ActionMovie represents the ActionMovie GraphQL type. This mapping is the responsibility of a TypeResolver . Tip: If your Java type names and GraphQL type names are the same, the DGS framework creates a `TypeResolver` automatically. No code needs to be added! Registering a Type Resolver If the name of your Java type and GraphQL type don't match, you need to provide a TypeResolver . A type resolver helps the framework map from concrete Java types to the correct object type in the schema. Use the @DgsTypeResolver annotation to register a type resolver. The annotation has a name property; set this to the name of the interface type or union type in the [GraphQL] schema. The resolver takes an object of the Java interface type, and returns a String which is the concrete object type of the instance as defined in the schema. The following is a type resolver for the Movie interface type introduced above: @DgsTypeResolver ( name = \"Movie\" ) public String resolveMovie ( Movie movie ) { if ( movie instanceof ScaryMovie ) { return \"ScaryMovie\" ; } else if ( movie instanceof ActionMovie ) { return \"ActionMovie\" ; } else { throw new RuntimeException ( \"Invalid type: \" + movie . getClass (). getName () + \" found in MovieTypeResolver\" ); } } You can add the @DgsTypeResolver annotation to any @DgsComponent class. This means you can either keep the type resolver in the same class as the data fetcher responsible for returning the data for this type, or you can create a separate class for it.","title":"Interfaces and Unions"},{"location":"advanced/type-resolvers-for-abstract-types/#registering-a-type-resolver","text":"If the name of your Java type and GraphQL type don't match, you need to provide a TypeResolver . A type resolver helps the framework map from concrete Java types to the correct object type in the schema. Use the @DgsTypeResolver annotation to register a type resolver. The annotation has a name property; set this to the name of the interface type or union type in the [GraphQL] schema. The resolver takes an object of the Java interface type, and returns a String which is the concrete object type of the instance as defined in the schema. The following is a type resolver for the Movie interface type introduced above: @DgsTypeResolver ( name = \"Movie\" ) public String resolveMovie ( Movie movie ) { if ( movie instanceof ScaryMovie ) { return \"ScaryMovie\" ; } else if ( movie instanceof ActionMovie ) { return \"ActionMovie\" ; } else { throw new RuntimeException ( \"Invalid type: \" + movie . getClass (). getName () + \" found in MovieTypeResolver\" ); } } You can add the @DgsTypeResolver annotation to any @DgsComponent class. This means you can either keep the type resolver in the same class as the data fetcher responsible for returning the data for this type, or you can create a separate class for it.","title":"Registering a Type Resolver"}]}